# Project Overview
Generated on: Sat Jul  5 01:14:48 EDT 2025

This script produces a comprehensive snapshot of all files in the ranked-choice project.
Sensitive data could be exposed, so protect 'project-overview.txt' accordingly.
---

## 1. Directory Structure

Below is the tree of files/folders (excluding .git, node_modules, dist, project-overview*):
```
.
├── .DS_Store
├── .env
├── .env.example
├── .gitignore
├── .nvmrc
├── docs
│   ├── agents.md
│   ├── architecture.md
│   └── glossary.md
├── package-lock.json
├── package.json
├── photo-select-here.sh
├── prompts
│   ├── default_prompt_olympia.hbs
│   ├── default_prompt.hbs
│   ├── field_notes_addon.txt
│   └── field_notes_second_pass.hbs
├── README.md
├── README.md.orig
├── scripts
│   └── generate-overview.sh
├── src
│   ├── chatClient.js
│   ├── chatClient.js.orig
│   ├── config.js
│   ├── fieldNotes.js
│   ├── hash.js
│   ├── imageSelector.js
│   ├── index.js
│   ├── orchestrator.js
│   └── orchestrator.js.orig
└── tests
    ├── chatClient.test.js
    ├── fieldNotes.test.js
    ├── imageSelector.test.js
    └── orchestrator.test.js

6 directories, 31 files
```

---

## 2. Full Content Dump
This section provides a textual representation of each file, skipping certain directories and file patterns.
PDF files are extracted as text if possible; binary files are noted but not shown in raw form.

### File: .
```
(File type is inode/directory — skipping raw dump.)
```

### File: ./photo-select-here.sh
```
#!/usr/bin/env bash
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
TARGET_DIR="$(pwd)"

# Load nvm if available
if [ -z "${NVM_DIR:-}" ]; then
  if [ -d "$HOME/.nvm" ]; then
    export NVM_DIR="$HOME/.nvm"
  fi
fi
if [ -s "$NVM_DIR/nvm.sh" ]; then
  . "$NVM_DIR/nvm.sh"
fi

# Use Node version from .nvmrc if nvm is available
if command -v nvm >/dev/null 2>&1; then
  if ! nvm use "$SCRIPT_DIR" >/dev/null 2>&1; then
    echo "nvm: Node $(cat "$SCRIPT_DIR/.nvmrc") not installed; using system node $(node --version)" >&2
  fi
fi

cd "$SCRIPT_DIR"

dir_specified=false
for arg in "$@"; do
  case "$arg" in
    -d|--dir|--dir=*|-d=*)
      dir_specified=true
      break
      ;;
  esac
done

if [ "$dir_specified" = true ]; then
  npx photo-select "$@"
else
  npx photo-select "$@" --dir "$TARGET_DIR"
fi
```

### File: ./.DS_Store
```
(File type is application/octet-stream — skipping raw dump.)
```

### File: ./tests
```
(File type is inode/directory — skipping raw dump.)
```

### File: ./tests/orchestrator.test.js
```
import { describe, it, expect, vi, beforeEach, afterEach } from "vitest";
import fs from "node:fs/promises";
import path from "node:path";
import os from "node:os";

vi.hoisted(() => {
  process.env.OPENAI_API_KEY = "test";
});

vi.mock("../src/chatClient.js", async () => {
  const actual = await vi.importActual("../src/chatClient.js");
  return {
    ...actual,
    chatCompletion: vi.fn(),
  };
});

import { chatCompletion } from "../src/chatClient.js";
import { triageDirectory } from "../src/orchestrator.js";

let tmpDir;
let promptFile;

beforeEach(async () => {
  tmpDir = await fs.mkdtemp(path.join(os.tmpdir(), "ps-test-"));
  await fs.writeFile(path.join(tmpDir, "1.jpg"), "a");
  await fs.writeFile(path.join(tmpDir, "2.jpg"), "b");
  promptFile = path.join(tmpDir, "prompt.txt");
  await fs.writeFile(promptFile, "prompt");
});

afterEach(async () => {
  vi.restoreAllMocks();
  await fs.rm(tmpDir, { recursive: true, force: true });
});

describe("triageDirectory", () => {
  it("moves files into keep and aside", async () => {
    chatCompletion.mockResolvedValueOnce(
      JSON.stringify({ keep: ["1.jpg"], aside: ["2.jpg"] })
    );
    await triageDirectory({
      dir: tmpDir,
      promptPath: promptFile,
      model: "test-model",
      recurse: false,
    });
    const keepPath = path.join(tmpDir, "_keep", "1.jpg");
    const asidePath = path.join(tmpDir, "_aside", "2.jpg");
    await expect(fs.stat(keepPath)).resolves.toBeTruthy();
    await expect(fs.stat(asidePath)).resolves.toBeTruthy();
    const level = path.join(tmpDir, "_level-001", "1.jpg");
    await expect(fs.stat(level)).resolves.toBeTruthy();
  });

  it("recurses into keep directory", async () => {
    chatCompletion
      .mockResolvedValueOnce(JSON.stringify({ keep: ["1.jpg"], aside: ["2.jpg"] }))
      .mockResolvedValueOnce(JSON.stringify({ keep: [], aside: ["1.jpg"] }));
    await triageDirectory({
      dir: tmpDir,
      promptPath: promptFile,
      model: "test-model",
      recurse: true,
    });
    expect(chatCompletion).toHaveBeenCalledTimes(2);
    const aside2 = path.join(tmpDir, "_keep", "_aside", "1.jpg");
    await expect(fs.stat(aside2)).resolves.toBeTruthy();
    const level2 = path.join(tmpDir, "_keep", "_level-002", "1.jpg");
    await expect(fs.stat(level2)).resolves.toBeTruthy();
  });

  it("recurses even when all images kept", async () => {
    chatCompletion
      .mockResolvedValueOnce(
        JSON.stringify({ keep: ["1.jpg", "2.jpg"], aside: [] })
      )
      .mockResolvedValueOnce(
        JSON.stringify({ keep: [], aside: ["1.jpg", "2.jpg"] })
      );
    await triageDirectory({
      dir: tmpDir,
      promptPath: promptFile,
      model: "test-model",
      recurse: true,
    });
    expect(chatCompletion).toHaveBeenCalledTimes(2);
    const aside2 = path.join(tmpDir, "_keep", "_aside", "2.jpg");
    await expect(fs.stat(aside2)).resolves.toBeTruthy();
  });

  it("updates field notes when enabled", async () => {
    chatCompletion.mockResolvedValueOnce(
      JSON.stringify({
        keep: ["1.jpg"],
        aside: ["2.jpg"],
        field_notes_diff: "--- a\n+++ b\n@@\n-Old\n+New",
      })
    );
    chatCompletion.mockResolvedValueOnce(
      JSON.stringify({ field_notes_md: "New" })
    );
    await triageDirectory({
      dir: tmpDir,
      promptPath: promptFile,
      model: "test-model",
      recurse: false,
      fieldNotes: true,
    });
    const noteFile = path.join(tmpDir, "_level-001", "field-notes.md");
    const content = await fs.readFile(noteFile, "utf8");
    expect(content).toMatch(/New/);
    expect(chatCompletion).toHaveBeenCalledTimes(2);
  });

  it("fails when field_notes_diff missing", async () => {
    chatCompletion.mockResolvedValueOnce(
      JSON.stringify({ keep: ["1.jpg"], aside: ["2.jpg"] })
    );
    await expect(
      triageDirectory({
        dir: tmpDir,
        promptPath: promptFile,
        model: "test-model",
        recurse: false,
        fieldNotes: true,
      })
    ).rejects.toThrow();
  });

  it("prints the prompt when showPrompt is true", async () => {
    chatCompletion.mockResolvedValueOnce(
      JSON.stringify({ keep: ["1.jpg"], aside: ["2.jpg"] })
    );
    const spy = vi.spyOn(console, "log").mockImplementation(() => {});
    await triageDirectory({
      dir: tmpDir,
      promptPath: promptFile,
      model: "test-model",
      recurse: false,
      showPrompt: 'full',
    });
    const calls = spy.mock.calls.some((c) => String(c[0]).includes("Prompt"));
    expect(calls).toBe(true);
    spy.mockRestore();
  });
});
```

### File: ./tests/chatClient.test.js
```
import { describe, it, expect, beforeAll, afterAll, vi } from "vitest";
import fs from "node:fs/promises";
import os from "node:os";
import path from "node:path";

let chatSpy;
let responsesSpy;

class MockNotFoundError extends Error {
  constructor(msg) {
    super(msg);
    this.status = 404;
  }
}

vi.mock("openai", () => {
  chatSpy = vi.fn();
  responsesSpy = vi.fn();
  return {
    OpenAI: vi.fn(() => ({
      chat: { completions: { create: chatSpy } },
      responses: { create: responsesSpy },
    })),
    NotFoundError: MockNotFoundError,
  };
});

let parseReply, buildMessages, buildInput, chatCompletion;
beforeAll(async () => {
  process.env.OPENAI_API_KEY = 'test-key';
  global.fetch = vi.fn(async () => ({ ok: true, json: async () => ({ data: [] }) }));
  ({ parseReply, buildMessages, buildInput, chatCompletion } = await import('../src/chatClient.js'));
});

afterAll(() => {
  global.fetch = undefined;
});

const files = [
  "/tmp/DSCF1234.jpg",
  "/tmp/DSCF5678.jpg",
  "/tmp/DSCF9012.jpg",
];

/** Basic parsing of keep/aside directives */
describe("parseReply", () => {
  it("classifies mentioned files and captures notes", () => {
    const reply = `DSCF1234.jpg -- keep - sharp shot\nSet aside: DSCF5678.jpg - blurry`;
    const { keep, aside, notes } = parseReply(reply, files);
    expect(keep).toContain(files[0]);
    expect(notes.get(files[0])).toMatch(/sharp/);
    expect(aside).toContain(files[1]);
    expect(notes.get(files[1])).toMatch(/blurry/);
  });

  it("leaves unmentioned files unclassified", () => {
    const reply = `keep: DSCF1234.jpg`;
    const { aside, keep, unclassified } = parseReply(reply, files);
    expect(keep).toContain(files[0]);
    expect(unclassified).toContain(files[1]);
    expect(unclassified).toContain(files[2]);
  });

  it("matches filenames when the reply omits prefixes", () => {
    const prefixed = [
      "/tmp/2020-01-01-DSCF1234.jpg",
      "/tmp/2020-01-01-DSCF5678.jpg",
    ];
    const reply = `DSCF1234.jpg -- keep\nSet aside: DSCF5678.jpg`;
    const { keep, aside } = parseReply(reply, prefixed);
    expect(keep).toContain(prefixed[0]);
    expect(aside).toContain(prefixed[1]);
  });

  it("parses JSON responses with reasoning", () => {
    const json = JSON.stringify({
      keep: { "DSCF1234.jpg": "good light" },
      aside: { "DSCF5678.jpg": "out of focus" },
    });
    const { keep, aside, notes } = parseReply(json, files);
    expect(keep).toContain(files[0]);
    expect(notes.get(files[0])).toMatch(/good light/);
    expect(aside).toContain(files[1]);
    expect(notes.get(files[1])).toMatch(/out of focus/);
    expect(aside).not.toContain(files[2]);
  });

  it("handles JSON wrapped in Markdown fences", () => {
    const fenced =
      '```json\n' +
      JSON.stringify({ keep: ["DSCF1234.jpg"], aside: ["DSCF5678.jpg"] }) +
      '\n```';
    const { keep, aside } = parseReply(fenced, files);
    expect(keep).toContain(files[0]);
    expect(aside).toContain(files[1]);
  });

  it("deduplicates files listed in both groups", () => {
    const reply = JSON.stringify({ keep: ["DSCF1234.jpg"], aside: ["DSCF1234.jpg"] });
    const { keep, aside } = parseReply(reply, files);
    expect(keep).toContain(files[0]);
    expect(aside).not.toContain(files[0]);
  });

  it("parses minutes and nested decision", () => {
    const reply = JSON.stringify({
      minutes: [{ speaker: "Curator", text: "looks good" }],
      decision: { keep: ["DSCF1234.jpg"], aside: ["DSCF5678.jpg"] },
    });
    const { keep, aside, minutes } = parseReply(reply, files);
    expect(minutes[0]).toMatch(/Curator/);
    expect(keep).toContain(files[0]);
    expect(aside).toContain(files[1]);
  });
});

/** Verify images are labelled in messages */
describe("buildMessages", () => {
  it("labels each image before the encoded data", async () => {
    const dir = await fs.mkdtemp(path.join(os.tmpdir(), "ps-msg-"));
    const img1 = path.join(dir, "1.jpg");
    const img2 = path.join(dir, "2.jpg");
    await fs.writeFile(img1, "a");
    await fs.writeFile(img2, "b");
    const [, user] = await buildMessages("prompt", [img1, img2]);
    expect(JSON.parse(user.content[1].text)).toEqual({ filename: "1.jpg" });
    expect(JSON.parse(user.content[3].text)).toEqual({ filename: "2.jpg" });
    await fs.rm(dir, { recursive: true, force: true });
  });

  it("includes people names when available", async () => {
    const dir = await fs.mkdtemp(path.join(os.tmpdir(), "ps-msg-"));
    const img1 = path.join(dir, "a.jpg");
    await fs.writeFile(img1, "a");
    global.fetch.mockResolvedValueOnce({
      ok: true,
      json: async () => ({ data: ["Alice", "Bob"] }),
    });
    const [, user] = await buildMessages("prompt", [img1]);
    const meta = JSON.parse(user.content[1].text);
    expect(meta.filename).toBe("a.jpg");
    expect(meta.people).toEqual(["Alice", "Bob"]);
    await fs.rm(dir, { recursive: true, force: true });
  });
});

describe("buildInput", () => {
  it("labels each image before the encoded data", async () => {
    const dir = await fs.mkdtemp(path.join(os.tmpdir(), "ps-in-"));
    const img1 = path.join(dir, "1.jpg");
    await fs.writeFile(img1, "a");
    const { input } = await buildInput("prompt", [img1]);
    const meta = JSON.parse(input[0].content[1].text);
    expect(meta).toEqual({ filename: "1.jpg" });
    await fs.rm(dir, { recursive: true, force: true });
  });

  it("includes people names when available", async () => {
    const dir = await fs.mkdtemp(path.join(os.tmpdir(), "ps-in-"));
    const img1 = path.join(dir, "a.jpg");
    await fs.writeFile(img1, "a");
    global.fetch.mockResolvedValueOnce({
      ok: true,
      json: async () => ({ data: ["Alice", "Bob"] }),
    });
    const { input } = await buildInput("prompt", [img1]);
    const meta = JSON.parse(input[0].content[1].text);
    expect(meta.filename).toBe("a.jpg");
    expect(meta.people).toEqual(["Alice", "Bob"]);
    await fs.rm(dir, { recursive: true, force: true });
  });
});

describe("chatCompletion", () => {
  let chatCompletion;

  beforeAll(async () => {
    ({ chatCompletion } = await import("../src/chatClient.js"));
  });

  it("falls back to responses when chat endpoint not supported", async () => {
    const errMsg =
      "This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?";
    chatSpy.mockRejectedValueOnce(new MockNotFoundError(errMsg));
    responsesSpy.mockResolvedValueOnce({ output_text: "ok" });
    const result = await chatCompletion({
      prompt: "p",
      images: [],
      model: "o3-pro",
      cache: false,
    });
    expect(responsesSpy).toHaveBeenCalled();
    const args = responsesSpy.mock.calls[0][0];
    expect(args.max_output_tokens).toBeTruthy();
    expect(result).toBe("ok");
  });
});
```

### File: ./tests/imageSelector.test.js
```
import { describe, it, expect } from "vitest";
import { pickRandom } from "../src/imageSelector.js";

describe("pickRandom", () => {
  it("returns no more than requested items", () => {
    const arr = [...Array(20).keys()];
    const result = pickRandom(arr, 10);
    expect(result.length).toBeLessThanOrEqual(10);
  });

  it("returns every item when array shorter than count", () => {
    const arr = [1, 2, 3];
    expect(pickRandom(arr, 10)).toHaveLength(3);
  });
});
```

### File: ./tests/fieldNotes.test.js
```
import { describe, it, expect, beforeEach, afterEach } from "vitest";
import fs from "node:fs/promises";
import os from "node:os";
import path from "node:path";
import { FieldNotesWriter } from "../src/fieldNotes.js";
import { createTwoFilesPatch } from "diff";

let dir;
let file;
let writer;

beforeEach(async () => {
  dir = await fs.mkdtemp(path.join(os.tmpdir(), "fn-test-"));
  file = path.join(dir, "field-notes.md");
  writer = new FieldNotesWriter(file);
  await writer.init();
});

afterEach(async () => {
  await fs.rm(dir, { recursive: true, force: true });
});

describe("FieldNotesWriter", () => {
  it("autolinks filenames and stamps", async () => {
    await writer.writeFull("See [a.jpg] and [b.png]");
    const md = await fs.readFile(file, "utf8");
    expect(md).toMatch(/\[a.jpg\]\(\.\/a.jpg\)/);
    expect(md).toMatch(/<!-- created:/);
    expect(md).toMatch(/<!-- updated:/);
  });

  it("applies diff patches", async () => {
    await writer.writeFull("Old\n");
    const diff = createTwoFilesPatch("a", "b", "Old\n", "New\n");
    await writer.applyDiff(diff);
    const md = await fs.readFile(file, "utf8");
    expect(md).toMatch(/New/);
  });
});
```

### File: ./README.md.orig
```
# photo‑select

A command‑line workflow that **selects 10 random images, asks ChatGPT which to “keep” or “set aside,”
moves the files accordingly, and then recurses until a directory is fully triaged.**

---

## Requirements

- Node 20+
- An OpenAI API key (via `OPENAI_API_KEY` or the `--api-key` flag)
  ```bash
  export OPENAI_API_KEY="sk‑..."
  ```

* macOS, Linux, or Windows‑WSL (uses only cross‑platform Node APIs)

## Installation

```bash
cd photo-select
nvm install   # obeys .nvmrc (Node 20.x)
nvm use
```

### 2  Configure your API key

1. Copy the template:

   ```bash
   cp .env.example .env
   ```

2. Paste your real key in `.env`:

   ```dotenv
   OPENAI_API_KEY=sk-...
   ```

No need to `export` the key in every shell—`dotenv` loads it automatically.

### 3  Install dependencies

```bash
npm install
chmod +x src/index.js    # fix permission error when running with npx
```

Invoke the CLI from the project directory using `npx`:

```bash
npx photo-select [--dir /path/to/images] [--prompt /path/to/prompt.txt] [--model gpt-4o] [--api-key sk-...] [--context /path/to/context.txt]
```

You can also install globally with `npm install -g` to run `photo-select` anywhere.

## Usage

```bash
# run from the project directory
npx photo-select [--dir /path/to/images] [--prompt /path/to/prompt.txt] [--model gpt-4o] [--api-key sk-...] [--context /path/to/context.txt]
# or, if installed globally:
photo-select [--dir /path/to/images] [--prompt /path/to/prompt.txt] [--model gpt-4o] [--api-key sk-...] [--context /path/to/context.txt]
```

Run `photo-select --help` to see all options.

### photo-select-here.sh

If you keep the repository cloned on your system, the `photo-select-here.sh`
script lets you run the CLI on whatever directory you're currently in without
typing `--dir` each time. Call it with the path to the script:

```bash
/path/to/photo-select/photo-select-here.sh [photo-select flags]
```

The script automatically:

1. Loads `nvm` and uses the Node version defined in `.nvmrc` if available.
2. Runs `npx photo-select` inside the repo.
3. Sets `--dir` to your current working directory unless you specify it
   explicitly.

All CLI flags—including `--api-key`, `--model`, and `--no-recurse`—can be passed
through to the script unchanged.

### Flags

| flag       | default                      | description                                     |
| ---------- | ---------------------------- | ----------------------------------------------- |
| `--dir`    | current directory            | Source directory containing images              |
| `--prompt` | `prompts/default_prompt.hbs` | Path to a custom prompt file                    |
| `--model`  | `gpt-4o`                | Any chat‑completion model id you have access to. Can also be set via `$PHOTO_SELECT_MODEL`. |
| `--api-key` | *(unset)*                  | OpenAI API key. Overrides `$OPENAI_API_KEY`. |
| `--curators` | *(unset)* | Comma-separated list of curator names used in the group transcript |
| `--context` | *(unset)* | Text file with exhibition context for the curators |
| `--no-recurse` | `false` | Process only the given directory without descending into `_keep` |
| `--field-notes` | `false` | Maintain a `field-notes.md` notebook in each `_level-NNN/` folder |
<<<<<<< HEAD
=======

Prompt files are Handlebars templates. Values such as `curators`, `context`, and
previous `fieldNotes` are injected when each request is built.
>>>>>>> 6261172d17708709d1665efa5b42b7df577809d0

### People metadata (optional)

Set `PHOTO_FILTER_API_BASE` to the base URL of your [photo‑filter](https://github.com/openhouse/photo-filter) service to include face‑tag data in the prompt. For each image the CLI fetches `/api/photos/by-filename/<filename>/persons` and sends a JSON blob like `{ "filename": "DSCF1234.jpg", "people": ["Alice", "Bob"] }` before the image itself. Results are cached per filename for the duration of the run.

Example:

```bash
PHOTO_FILTER_API_BASE=http://localhost:3000 \
/path/to/photo-select/photo-select-here.sh --api-key sk-... --model o3 \
  --curators "Ingeborg Gerdes, Alexandra Munroe, Mandy Steinback, Kendell Harbin, Erin Zona, Madeline Gallucci, Deborah Treisman" \
  --context /path/to/info.md
```

## Supported OpenAI models

The CLI calls the Chat Completions API and automatically switches to `/v1/responses` if a model only supports that endpoint. Any vision-capable chat model listed on OpenAI's [models](https://platform.openai.com/docs/models) page should work, including:


* **GPT‑4.1 family** – `gpt-4.1`, `gpt-4.1-mini`, and `gpt-4.1-nano`
* **GPT‑4o family** – `gpt-4o` (default), `gpt-4o-mini`, `gpt-4o-audio-preview`,
  `gpt-4o-mini-audio-preview`, `gpt-4o-realtime-preview`,
  `gpt-4o-mini-realtime-preview`, `gpt-4o-search-preview`, and
  `gpt-4o-mini-search-preview`
* **o‑series reasoning models** – `o4-mini`, `o3`, `o3-pro` *(responses API)*,
  `o3-mini`, `o1`, `o1-pro`, and the deprecated `o1-mini`
* **Other vision models** – `gpt-4-turbo`, `gpt-4.5-preview` *(deprecated)*. The
  `gpt-4-vision-preview` model has been removed.

Example output of `openai api models.list`:

```text
gpt-4.1-nano
gpt-4.1-nano-2025-04-14
gpt-4.5-preview
gpt-4.5-preview-2025-02-27
gpt-4o
gpt-4o-2024-05-13
gpt-4o-2024-08-06
gpt-4o-2024-11-20
gpt-4o-audio-preview
gpt-4o-audio-preview-2024-10-01
gpt-4o-audio-preview-2024-12-17
gpt-4o-audio-preview-2025-06-03
gpt-4o-mini
gpt-4o-mini-2024-07-18
gpt-4o-mini-audio-preview
gpt-4o-mini-audio-preview-2024-12-17
gpt-4o-mini-realtime-preview
gpt-4o-mini-realtime-preview-2024-12-17
gpt-4o-mini-search-preview
gpt-4o-mini-search-preview-2025-03-11
gpt-4o-realtime-preview
gpt-4o-realtime-preview-2024-10-01
gpt-4o-realtime-preview-2024-12-17
gpt-4o-realtime-preview-2025-06-03
gpt-4o-search-preview
gpt-4o-search-preview-2025-03-11
o1
o1-2024-12-17
o1-mini
o1-mini-2024-09-12
o1-pro
o1-pro-2025-03-19
o3
o3-2025-04-16
o3-mini
o3-mini-2025-01-31
o3-pro
o3-pro-2025-06-10
o4-mini
o4-mini-2025-04-16
```
These names match the model ids provided by the OpenAI Node SDK, as seen in its
[type definitions](node_modules/openai/resources/beta/assistants.d.ts).

Models in the `o` series use the new `max_completion_tokens` parameter instead of
the deprecated `max_tokens`. When the CLI falls back to the Responses API, that
option is called `max_output_tokens`. Both are handled automatically based on
the model you specify.

### Estimated costs

The cost depends on the number of tokens generated from your images. Roughly
speaking, a single 6240 × 4160 image is about 8,000 input tokens. Processing the
full 315‑photo set therefore uses about 2.5 million input tokens plus roughly
250k output tokens.

Approximate price per run:

| model          | input $/1M | output $/1M | est. cost on 315 photos |
| -------------- | ---------- | ----------- | ---------------------- |
| `gpt-4.1`      | $2.00      | $8.00       | ~$7 |
| `gpt-4.1-mini` | $0.40      | $1.60       | ~$1.4 |
| `gpt-4.1-nano` | $0.10      | $0.40       | ~$0.35 |
| `o4-mini`      | $1.10      | $4.40       | ~$3.85 |
| `o3`           | $2.00      | $8.00       | ~$7 |
| `o3-pro`       | $20.00     | $80.00      | ~$70 |
| `o3-mini`      | $1.10      | $4.40       | ~$3.85 |
| `o1`           | $15.00     | $60.00      | ~$52.5 |
| `o1-pro`       | $150.00    | $600.00     | ~$525 |
| `gpt-4o`       | $2.50      | $10.00      | ~$9 |
| `gpt-4o-mini`  | $0.15      | $0.60       | ~$0.55 |
| `gpt-4-turbo`  | $10.00     | $30.00      | ~$33 |
| `gpt-4.5-preview`      | $75.00     | $150.00     | ~$225 |
| `gpt-4`        | $30.00     | $60.00      | ~$90 |

These figures are approximate and based on current
[OpenAI pricing](https://openai.com/pricing). Actual costs will vary with output
length and any image resizing.

### Measuring model quality

There is no public benchmark for photo triage, so the best approach is to
create a small validation set—say 30–50 images—with your own "keep" vs. "aside"
labels. Run the CLI on this set with different models (using `--model` or
`PHOTO_SELECT_MODEL`) and save the results. Comparing those decisions to your
labels lets you compute precision, recall, and F1‑score for each model. Repeating
the process on multiple batches will highlight which model gives the most
consistent choices.

The tool creates `_keep` and `_aside` sub‑folders inside every directory it touches.

### Example: A/B testing models

You can duplicate a directory of images and run the script on each copy with
different `--model` values. Each run writes its own `_keep` and `_aside` folders
so you can compare the results side by side.

```bash
# prepare two identical folders
mkdir trial-gpt-4o trial-gpt-4.5-preview
cp /path/to/source/*.jpg trial-gpt-4o/
cp /path/to/source/*.jpg trial-gpt-4.5-preview/

# run with GPT‑4o
/path/to/photo-select/photo-select-here.sh --model gpt-4o --dir trial-gpt-4o --api-key sk-... --context /path/to/context.txt

# run with GPT‑4.5-preview
/path/to/photo-select/photo-select-here.sh --model gpt-4.5-preview --dir trial-gpt-4.5-preview --api-key sk-... --context /path/to/context.txt
```

If you see repeated `OpenAI error (404)` messages, your API key may not have
access to that model or the id is misspelled. Check `openai models:list` to
confirm which ids are enabled for your account. Models that require the
`/v1/responses` endpoint—such as `o1-pro` or `o3-pro`—are automatically routed
through that API, so no extra flags are needed.


## Recursion logic

1. Pick up to 10 random images (all common photo extensions).
2. Send them to ChatGPT with the prompt (filenames included).
3. ChatGPT replies with meeting minutes summarising a short discussion among the curators, followed by a JSON object indicating which files to keep or set aside and why.
4. Parse that JSON to determine which files were explicitly labeled `keep` or `aside` and capture any notes about each image.
5. Move those files to the corresponding sub‑folders and write a text file containing the notes next to each image. Files omitted from the decision block remain in place for the next batch so the model can review them again. Meeting minutes are saved as `minutes-<timestamp>.txt` in the directory.
6. Re‑run the algorithm on the newly created `_keep` folder (unless `--no-recurse`).
   If every photo at a level is kept or every photo is set aside, recursion stops early.
7. On the first pass of each level a `_level-XXX` folder is created next to `_keep` and `_aside` containing a snapshot of the images originally present.
8. If `--field-notes` is enabled, the same directory holds a `field-notes.md` notebook updated by the curators in two passes.
9. Stop when a directory has zero unclassified images.

### JSON mode

The OpenAI request uses `response_format: { type: "json_object" }` so the
assistant replies with strict JSON. This avoids needing to strip Markdown
fences and guarantees parseable output.
The CLI allows up to 4096 tokens in each reply (see `MAX_RESPONSE_TOKENS` in
`src/chatClient.js`) so the minutes and JSON decision block are returned in
full.

## Caching

Responses from OpenAI are cached under a `.cache` directory using a hash of the
prompt, model, and file metadata. Subsequent runs with the same inputs reuse the
saved reply instead of hitting the API.

## Testing

```bash
npm test
```

The **Vitest** suite covers random selection, safe moves, and response‑parsing.

---

## Development tips

- Use `nvm use` in every new shell (or add a shell hook).
- Need a different Node version temporarily? `nvm exec 18 npm test`.
- `.env` is ignored by git—share `.env.example` instead.

---

## Inspiration

Built to replace a manual workflow that relied on Finder tags and the ChatGPT web UI.
Now everything—random choice, conversation, and file moves—happens automatically in the shell.
```

### File: ./docs
```
(File type is inode/directory — skipping raw dump.)
```

### File: ./docs/architecture.md
```
# Photo-Select – Architecture at a Glance
├─ src/                     ← runtime code
│  ├─ cli/                  ← thin Cmd+Option wrappers (index.js only calls into here)
│  ├─ core/                 ← domain logic (triageDirectory, FieldNotesWriter, etc.)
│  ├─ adapters/             ← I/O edges: chatClient (OpenAI), imageSelector (fs), peopleApi
│  └─ templates/            ← compiled HBS strings (wired up at build time)
├─ prompts/                 ← raw *.hbs & *.txt prompt sources
├─ docs/                    ← human-readable guides
│  ├─ architecture.md       ← **you are here**
│  ├─ agents.md             ← prompt-engineering contract (see below)
│  └─ glossary.md           ← canonical vocabulary
└─ tests/                   ← Vitest; keep unit & integration separate

## Key patterns
1. **HBS as “configuration, not code.”**
Treat every Handlebars template exactly like a JSON schema: no business logic inside, just placeholders. The runtime helper (`renderTemplate`) already enforces this.
2. **Adapters > Services > Pure functions.**
   - Adapters know about network, fs, OpenAI.
   - Services orchestrate adapters (e.g. `triageDirectory`).
   - Pure functions (e.g. `parseReply`) have zero side-effects and live under `src/core/`.
3. **Prompt lifecycle.**

```
template.hbs  →  renderTemplate(data)  →  prompt string
prompt string + images  →  chatClient  →  JSON reply
JSON reply  →  pure parser  →  move/apply/write
```

4. **Second-pass prompts.**
Strip the addon rules before nesting the original prompt to avoid instruction collision (already noted; implementation pending).
5. **Cache keys are contracts.**
If you touch anything that changes the cacheKey hash inputs, bump the cache-version prefix so old replies don’t leak into new logic.
```

### File: ./docs/glossary.md
```
# Photo-Select – Core Concepts

**Batch** – A set of ≤ 10 images funnelled through a single prompt/response round.

**Curators (Agents)** – Named voices who appear in the “minutes” section.  
They function as *thinking styles* the LLM can adopt (e.g. technical, poetic).

**Facilitator** – Jamie’s voice; she does *not* appear in JSON minutes but frames the session.

**Minutes** – A diarised conversation (array of `{ speaker, text }`) capturing how consensus was reached.

**Decision Block** – Strict JSON `{ keep, aside }`, optionally enriched with `field_notes_diff`.

**Field Notes** – Markdown notebook (`field-notes.md`) evolving in two passes.  
First pass returns a diff; second pass resolves to the full document.

**Level Directory** – `_level-NNN/` snapshot preserving original files for provenance.

**Adapter** – Boundary module that converts between external systems (OpenAI, filesystem) and pure functions.

**Service** – Coordinator function (e.g. `triageDirectory`) that expresses application use-cases in terms of adapters & pure functions.

**Prompt Snapshot** – A `.prompt.txt` file stored in each level directory containing the exact text sent to the model for reproducibility.
```

### File: ./docs/agents.md
```
# Agents Contract  · Photo-Select

---
schema_version: 2
updated: 2025-07-05
---

This document **must be loaded by any code-assistant before touching the repo.**
It defines binding constraints for all prompt templates. Any change here **must**
increment `schema_version` and update tests.

## 1  Synthetic Voices
| Id | Role in Minutes | Style | Required Closing Question | May Edit Field-Notes? | Psychology Cue | Example Utterance |
|----|-----------------|-------|---------------------------|-----------------------|----------------|------------------|
| *Curator-A* | “Ingeborg Gerdes” | aesthetic, formal | optional | yes | reflective | "Notice the diffuse glow here." |
| *Curator-B* | “Alexandra Munroe” | scholarly, contextual | optional | yes | analytic | "This recalls earlier site works." |
| … | … | … | optional | yes | varied | "…" |
| Facilitator | “Jamie (off-stage)” | session framing only | never | no | pragmatic | "Next batch coming up." |

### schema_duty per persona
*Curator-A*
schema_duty: ensures field_notes_diff syntax is valid unified diff.

*Curator-B*
schema_duty: checks contextual accuracy in minutes.

*Facilitator*
schema_duty: none; outside minutes/decisions.

> **Rule:** Minutes **always** stay within the personas above.
> **Rule:** Decisions follow minutes, never interwoven.
> **Rule:** Minutes end with a forward-looking question from one curator.

## 2  Two-Pass Field-Notes Workflow
| Phase | LLM Output | Parser expectation |
|-------|------------|--------------------|
| 1st pass | `field_notes_diff` **OR** `field_notes_md` | `expectFieldNotesDiff = true` |
| 2nd pass (if diff) | `field_notes_md` | `expectFieldNotesMd = true` |

The *same* curator voices and context must be provided in both passes.

## 3  Prompt Template Placeholders
| Placeholder | Source |
|-------------|--------|
| `{{curators}}` | CLI `--curators` flag |
| `{{context}}` | CLI `--context` file |
| `{{fieldNotes}}` | previous notebook text |

## 4  Coding Standards
* No business logic in `.hbs`; keep them data-driven.  
* Pure functions live under `src/core/`; they receive **all** inputs explicitly.  
* Mutations to `cacheKey` MUST bump version (prefix string).

*(Last edited: 2025-07-05)*
```

### File: ./README.md
```
# photo‑select

A command‑line workflow that **selects 10 random images, asks ChatGPT which to “keep” or “set aside,”
moves the files accordingly, and then recurses until a directory is fully triaged.**

## Documentation Quick-Links
* [Architecture guide](docs/architecture.md)
* [Agents contract](docs/agents.md)
* [Glossary](docs/glossary.md)


---

## Requirements

- Node 20+
- An OpenAI API key (via `OPENAI_API_KEY` or the `--api-key` flag)
  ```bash
  export OPENAI_API_KEY="sk‑..."
  ```

* macOS, Linux, or Windows‑WSL (uses only cross‑platform Node APIs)

## Installation

```bash
cd photo-select
nvm install   # obeys .nvmrc (Node 20.x)
nvm use
```

### 2  Configure your API key

1. Copy the template:

   ```bash
   cp .env.example .env
   ```

2. Paste your real key in `.env`:

   ```dotenv
   OPENAI_API_KEY=sk-...
   ```

No need to `export` the key in every shell—`dotenv` loads it automatically.

### 3  Install dependencies

```bash
npm install
chmod +x src/index.js    # fix permission error when running with npx
```

Invoke the CLI from the project directory using `npx`:

```bash
npx photo-select [--dir /path/to/images] [--prompt /path/to/prompt.txt] [--model gpt-4o] [--api-key sk-...] [--context /path/to/context.txt]
```

You can also install globally with `npm install -g` to run `photo-select` anywhere.

## Usage

```bash
# run from the project directory
npx photo-select [--dir /path/to/images] [--prompt /path/to/prompt.txt] [--model gpt-4o] [--api-key sk-...] [--context /path/to/context.txt]
# or, if installed globally:
photo-select [--dir /path/to/images] [--prompt /path/to/prompt.txt] [--model gpt-4o] [--api-key sk-...] [--context /path/to/context.txt]
```

Run `photo-select --help` to see all options.

### photo-select-here.sh

If you keep the repository cloned on your system, the `photo-select-here.sh`
script lets you run the CLI on whatever directory you're currently in without
typing `--dir` each time. Call it with the path to the script:

```bash
/path/to/photo-select/photo-select-here.sh [photo-select flags]
```

The script automatically:

1. Loads `nvm` and uses the Node version defined in `.nvmrc` if available.
2. Runs `npx photo-select` inside the repo.
3. Sets `--dir` to your current working directory unless you specify it
   explicitly.

All CLI flags—including `--api-key`, `--model`, and `--no-recurse`—can be passed
through to the script unchanged.

### Flags

| flag       | default                      | description                                     |
| ---------- | ---------------------------- | ----------------------------------------------- |
| `--dir`    | current directory            | Source directory containing images              |
| `--prompt` | `prompts/default_prompt.hbs` | Path to a custom prompt file                    |
| `--model`  | `gpt-4o`                | Any chat‑completion model id you have access to. Can also be set via `$PHOTO_SELECT_MODEL`. |
| `--api-key` | *(unset)*                  | OpenAI API key. Overrides `$OPENAI_API_KEY`. |
| `--curators` | *(unset)* | Comma-separated list of curator names used in the group transcript |
| `--context` | *(unset)* | Text file with exhibition context for the curators |
| `--no-recurse` | `false` | Process only the given directory without descending into `_keep` |
| `--field-notes` | `false` | Maintain a `field-notes.md` notebook in each `_level-NNN/` folder |
| `--show-prompt` | `false` | Output the prompt (`full`, `hash`, or `preview`) before each API call |

Each batch stores the final text sent to the model as `.prompt.txt` in its `_level-NNN/` folder. Use `--show-prompt=hash` to print only a SHA-256 of the prompt, or `--show-prompt=preview` to print the first 100 lines followed by the hash.

Prompt files are Handlebars templates. Values such as `curators`, `context`, and
previous `fieldNotes` are injected when each request is built.

### People metadata (optional)

Set `PHOTO_FILTER_API_BASE` to the base URL of your [photo‑filter](https://github.com/openhouse/photo-filter) service to include face‑tag data in the prompt. For each image the CLI fetches `/api/photos/by-filename/<filename>/persons` and sends a JSON blob like `{ "filename": "DSCF1234.jpg", "people": ["Alice", "Bob"] }` before the image itself. Results are cached per filename for the duration of the run.

Example:

```bash
PHOTO_FILTER_API_BASE=http://localhost:3000 \
/path/to/photo-select/photo-select-here.sh --api-key sk-... --model o3 \
  --curators "Ingeborg Gerdes, Alexandra Munroe, Mandy Steinback, Kendell Harbin, Erin Zona, Madeline Gallucci, Deborah Treisman" \
  --context /path/to/info.md
```

## Supported OpenAI models

The CLI calls the Chat Completions API and automatically switches to `/v1/responses` if a model only supports that endpoint. Any vision-capable chat model listed on OpenAI's [models](https://platform.openai.com/docs/models) page should work, including:


* **GPT‑4.1 family** – `gpt-4.1`, `gpt-4.1-mini`, and `gpt-4.1-nano`
* **GPT‑4o family** – `gpt-4o` (default), `gpt-4o-mini`, `gpt-4o-audio-preview`,
  `gpt-4o-mini-audio-preview`, `gpt-4o-realtime-preview`,
  `gpt-4o-mini-realtime-preview`, `gpt-4o-search-preview`, and
  `gpt-4o-mini-search-preview`
* **o‑series reasoning models** – `o4-mini`, `o3`, `o3-pro` *(responses API)*,
  `o3-mini`, `o1`, `o1-pro`, and the deprecated `o1-mini`
* **Other vision models** – `gpt-4-turbo`, `gpt-4.5-preview` *(deprecated)*. The
  `gpt-4-vision-preview` model has been removed.

Example output of `openai api models.list`:

```text
gpt-4.1-nano
gpt-4.1-nano-2025-04-14
gpt-4.5-preview
gpt-4.5-preview-2025-02-27
gpt-4o
gpt-4o-2024-05-13
gpt-4o-2024-08-06
gpt-4o-2024-11-20
gpt-4o-audio-preview
gpt-4o-audio-preview-2024-10-01
gpt-4o-audio-preview-2024-12-17
gpt-4o-audio-preview-2025-06-03
gpt-4o-mini
gpt-4o-mini-2024-07-18
gpt-4o-mini-audio-preview
gpt-4o-mini-audio-preview-2024-12-17
gpt-4o-mini-realtime-preview
gpt-4o-mini-realtime-preview-2024-12-17
gpt-4o-mini-search-preview
gpt-4o-mini-search-preview-2025-03-11
gpt-4o-realtime-preview
gpt-4o-realtime-preview-2024-10-01
gpt-4o-realtime-preview-2024-12-17
gpt-4o-realtime-preview-2025-06-03
gpt-4o-search-preview
gpt-4o-search-preview-2025-03-11
o1
o1-2024-12-17
o1-mini
o1-mini-2024-09-12
o1-pro
o1-pro-2025-03-19
o3
o3-2025-04-16
o3-mini
o3-mini-2025-01-31
o3-pro
o3-pro-2025-06-10
o4-mini
o4-mini-2025-04-16
```
These names match the model ids provided by the OpenAI Node SDK, as seen in its
[type definitions](node_modules/openai/resources/beta/assistants.d.ts).

Models in the `o` series use the new `max_completion_tokens` parameter instead of
the deprecated `max_tokens`. When the CLI falls back to the Responses API, that
option is called `max_output_tokens`. Both are handled automatically based on
the model you specify.

### Estimated costs

The cost depends on the number of tokens generated from your images. Roughly
speaking, a single 6240 × 4160 image is about 8,000 input tokens. Processing the
full 315‑photo set therefore uses about 2.5 million input tokens plus roughly
250k output tokens.

Approximate price per run:

| model          | input $/1M | output $/1M | est. cost on 315 photos |
| -------------- | ---------- | ----------- | ---------------------- |
| `gpt-4.1`      | $2.00      | $8.00       | ~$7 |
| `gpt-4.1-mini` | $0.40      | $1.60       | ~$1.4 |
| `gpt-4.1-nano` | $0.10      | $0.40       | ~$0.35 |
| `o4-mini`      | $1.10      | $4.40       | ~$3.85 |
| `o3`           | $2.00      | $8.00       | ~$7 |
| `o3-pro`       | $20.00     | $80.00      | ~$70 |
| `o3-mini`      | $1.10      | $4.40       | ~$3.85 |
| `o1`           | $15.00     | $60.00      | ~$52.5 |
| `o1-pro`       | $150.00    | $600.00     | ~$525 |
| `gpt-4o`       | $2.50      | $10.00      | ~$9 |
| `gpt-4o-mini`  | $0.15      | $0.60       | ~$0.55 |
| `gpt-4-turbo`  | $10.00     | $30.00      | ~$33 |
| `gpt-4.5-preview`      | $75.00     | $150.00     | ~$225 |
| `gpt-4`        | $30.00     | $60.00      | ~$90 |

These figures are approximate and based on current
[OpenAI pricing](https://openai.com/pricing). Actual costs will vary with output
length and any image resizing.

### Measuring model quality

There is no public benchmark for photo triage, so the best approach is to
create a small validation set—say 30–50 images—with your own "keep" vs. "aside"
labels. Run the CLI on this set with different models (using `--model` or
`PHOTO_SELECT_MODEL`) and save the results. Comparing those decisions to your
labels lets you compute precision, recall, and F1‑score for each model. Repeating
the process on multiple batches will highlight which model gives the most
consistent choices.

The tool creates `_keep` and `_aside` sub‑folders inside every directory it touches.

### Example: A/B testing models

You can duplicate a directory of images and run the script on each copy with
different `--model` values. Each run writes its own `_keep` and `_aside` folders
so you can compare the results side by side.

```bash
# prepare two identical folders
mkdir trial-gpt-4o trial-gpt-4.5-preview
cp /path/to/source/*.jpg trial-gpt-4o/
cp /path/to/source/*.jpg trial-gpt-4.5-preview/

# run with GPT‑4o
/path/to/photo-select/photo-select-here.sh --model gpt-4o --dir trial-gpt-4o --api-key sk-... --context /path/to/context.txt

# run with GPT‑4.5-preview
/path/to/photo-select/photo-select-here.sh --model gpt-4.5-preview --dir trial-gpt-4.5-preview --api-key sk-... --context /path/to/context.txt
```

If you see repeated `OpenAI error (404)` messages, your API key may not have
access to that model or the id is misspelled. Check `openai models:list` to
confirm which ids are enabled for your account. Models that require the
`/v1/responses` endpoint—such as `o1-pro` or `o3-pro`—are automatically routed
through that API, so no extra flags are needed.


## Recursion logic

1. Pick up to 10 random images (all common photo extensions).
2. Send them to ChatGPT with the prompt (filenames included).
3. ChatGPT replies with meeting minutes summarising a short discussion among the curators, followed by a JSON object indicating which files to keep or set aside and why.
4. Parse that JSON to determine which files were explicitly labeled `keep` or `aside` and capture any notes about each image.
5. Move those files to the corresponding sub‑folders and write a text file containing the notes next to each image. Files omitted from the decision block remain in place for the next batch so the model can review them again. Meeting minutes are saved as `minutes-<timestamp>.txt` in the directory.
6. Re‑run the algorithm on the newly created `_keep` folder (unless `--no-recurse`).
   If every photo at a level is kept or every photo is set aside, recursion stops early.
7. On the first pass of each level a `_level-XXX` folder is created next to `_keep` and `_aside` containing a snapshot of the images originally present.
8. If `--field-notes` is enabled, the same directory holds a `field-notes.md` notebook updated by the curators in two passes.
9. Stop when a directory has zero unclassified images.

### JSON mode

The OpenAI request uses `response_format: { type: "json_object" }` so the
assistant replies with strict JSON. This avoids needing to strip Markdown
fences and guarantees parseable output.
The CLI allows up to 4096 tokens in each reply (see `MAX_RESPONSE_TOKENS` in
`src/chatClient.js`) so the minutes and JSON decision block are returned in
full.

## Caching

Responses from OpenAI are cached under a `.cache` directory using a hash of the
prompt, model, and file metadata. Subsequent runs with the same inputs reuse the
saved reply instead of hitting the API.

## Testing

```bash
npm test
```

The **Vitest** suite covers random selection, safe moves, and response‑parsing.

---

## Development tips

- Use `nvm use` in every new shell (or add a shell hook).
- Need a different Node version temporarily? `nvm exec 18 npm test`.
- `.env` is ignored by git—share `.env.example` instead.

---

## Inspiration

Built to replace a manual workflow that relied on Finder tags and the ChatGPT web UI.
Now everything—random choice, conversation, and file moves—happens automatically in the shell.
```

### File: ./.gitignore
```
# dependencies & build artifacts
/node_modules
/.env
.DS_Store
```

### File: ./package.json
```
{
  "name": "photo-select",
  "version": "0.1.1",
  "description": "CLI tool to recursively triage fine‑art photos with ChatGPT 4.5",
  "type": "module",
  "bin": {
    "photo-select": "./src/index.js"
  },
  "scripts": {
    "start": "photo-select",
    "test": "vitest run"
  },
  "engines": {
    "node": ">=20"
  },
  "author": "Jamie Burkart",
  "license": "MIT",
  "dependencies": {
    "commander": "^12.0.0",
    "diff": "^8.0.2",
    "dotenv": "^16.5.0",
    "handlebars": "^4.7.8",
    "openai": "^4.0.0",
    "sharp": "^0.34.2",
    "zod": "^3.23.8"
  },
  "devDependencies": {
    "vitest": "^1.5.0"
  }
}
```

### File: ./.nvmrc
```
20
```

### File: ./scripts
```
(File type is inode/directory — skipping raw dump.)
```

### File: ./scripts/generate-overview.sh
```
#!/usr/bin/env bash
##############################################################################
# generate-overview.sh
#
# Generates a text-based overview of the 'ranked-choice' project and saves
# the result to 'project-overview.txt'. It includes:
#   1. A directory structure overview (via 'tree' or 'ls -R')
#   2. A single-pass approach to display textual contents of files:
#      - PDFs extracted as text, using pdftotext or OCR fallback.
#      - Plain text or JSON/XML files shown in full (with an exception
#        for 'generatedOutputs.json', which is summarized).
#      - Binary files noted but not shown in raw form.
#   3. Skips certain known directories and file patterns:
#      - .git, node_modules, dist, project-overview*, etc.
#   4. Concludes with a basic system report and optional Ollama models listing.
#
# This version uses GNU coreutils' gshuf to randomly sample objects from
# JSON arrays when processing 'generatedOutputs.json'.
#
# Usage:
#   ./scripts/generate-overview.sh
#
# Requirements:
#   - tree (optional, for nicer directory listing)
#   - pdftotext (Poppler) or Xpdf (for PDF text extraction)
#   - tesseract (optional, for OCR fallback if PDF has no embedded text)
#   - ollama (optional, to list installed local models)
#   - jq (for JSON processing)
#   - GNU coreutils (for gshuf, which may be installed via Homebrew coreutils)
#
# WARNING:
#   This script can expose sensitive data in 'project-overview.txt'.
#   Handle the resulting file with care!
##############################################################################

OUTPUT_FILE="project-overview.txt"

# Direct all script output into 'project-overview.txt'
exec > "$OUTPUT_FILE" 2>&1

echo "# Project Overview"
echo "Generated on: $(date)"
echo ""
echo "This script produces a comprehensive snapshot of all files in the ranked-choice project."
echo "Sensitive data could be exposed, so protect 'project-overview.txt' accordingly."
echo "---"
echo ""

##############################################################################
# 1) Directory Structure Overview
##############################################################################
echo "## 1. Directory Structure"
echo ""
if command -v tree >/dev/null 2>&1; then
  echo "Below is the tree of files/folders (excluding .git, node_modules, dist, project-overview*):"
  echo '```'
  tree -a -I ".git|.cache|node_modules|dist|project-overview*" .
  echo '```'
else
  echo "Below is the 'ls -R' style listing of files/folders (excluding .git, node_modules, dist, project-overview*)."
  echo "Install 'tree' for a more visual directory listing."
  echo '```'
  find . \
    -path "*/.git" -prune -o \
    -path "*/.cache" -prune -o \
    -path "*/node_modules" -prune -o \
    -path "*/dist" -prune -o \
    -name "project-overview*" -prune -o \
    -print
  echo '```'
fi

echo ""
echo "---"
echo ""

##############################################################################
# Function to summarize generatedOutputs.json
# - Prints stats on modelName, stage, promptUsed, timestamp range, and a random sample.
##############################################################################
summarize_generated_outputs() {
  local filePath="$1"

  echo "### Summaries for $filePath"
  echo ""

  if ! command -v jq >/dev/null 2>&1; then
    echo "(jq not installed. Cannot provide advanced summary. Only file note.)"
    return
  fi

  # Check file size and object count
  local fileSize
  fileSize=$(stat -c%s "$filePath" 2>/dev/null || stat -f%z "$filePath" 2>/dev/null)
  echo "File size (bytes): $fileSize"

  local totalCount
  totalCount=$(jq '. | length' "$filePath" 2>/dev/null)
  echo "Total JSON objects in $filePath: $totalCount"
  echo ""

  if [ "$totalCount" -eq 0 ] || [ "$totalCount" = "null" ]; then
    echo "(File is empty or not valid JSON.)"
    return
  fi

  # modelName distribution
  echo "#### modelName distribution (sorted by count desc):"
  jq 'group_by(.modelName)
      | map({modelName: .[0].modelName, count: length})
      | sort_by(.count)
      | reverse' "$filePath"
  echo ""

  # stage distribution
  echo "#### stage distribution (sorted by count desc):"
  jq 'group_by(.stage)
      | map({stage: .[0].stage, count: length})
      | sort_by(.count)
      | reverse' "$filePath"
  echo ""

  ## promptUsed distribution
  #echo "#### promptUsed distribution (sorted by count desc):"
  #jq 'group_by(.promptUsed)
  #    | map({promptUsed: .[0].promptUsed, count: length})
  #    | sort_by(.count)
  #    | reverse' "$filePath"
  #echo ""

  # timestamp min/max
  echo "#### timestamp range:"
  local minTimestamp
  local maxTimestamp
  minTimestamp=$(jq 'min_by(.timestamp) | .timestamp' "$filePath")
  maxTimestamp=$(jq 'max_by(.timestamp) | .timestamp' "$filePath")
  echo "Min timestamp: $minTimestamp"
  echo "Max timestamp: $maxTimestamp"
  echo ""

  # Sample size
  local sampleSize=1
  if [ "$totalCount" -lt "$sampleSize" ]; then
    sampleSize="$totalCount"
  fi

  echo "#### Sample $sampleSize object(s) from $filePath:"
  if command -v gshuf >/dev/null 2>&1; then
    # Use gshuf to randomize: output each element on one line, then shuffle, then reassemble into an array.
    jq -c '.[]' "$filePath" | gshuf -n "$sampleSize" | jq -s '.'
  else
    echo "(gshuf not found, falling back to last $sampleSize items.)"
    jq --arg c "$sampleSize" '. | (.[-($c|tonumber):])' "$filePath"
  fi
  echo ""
}

##############################################################################
# 2) Single-Pass: Full Content Dump of Files
##############################################################################
echo "## 2. Full Content Dump"
echo "This section provides a textual representation of each file, skipping certain directories and file patterns."
echo "PDF files are extracted as text if possible; binary files are noted but not shown in raw form."
echo ""

# Directories to skip anywhere in the repo
SKIP_DIRS=(.git node_modules dist .vscode .cache)

# File patterns to skip
SKIP_FILES=("*.lock" "yarn.lock" "package-lock.json" ".env" "project-overview*")

FIND_CMD=(find .)

# Exclude skip directories (anywhere in the path)
for dir in "${SKIP_DIRS[@]}"; do
  FIND_CMD+=( -path "*/$dir" -prune -o )
done

# Only proceed with -type f after pruning directories
FIND_CMD+=( -type f )

# Exclude skip files
for pattern in "${SKIP_FILES[@]}"; do
  FIND_CMD+=( \( -iname "$pattern" \) -prune -o )
done

FIND_CMD+=( -print )

mapfile -t ALL_FILES < <("${FIND_CMD[@]}" 2>/dev/null)

if [ ${#ALL_FILES[@]} -eq 0 ]; then
  echo "No files found based on skip rules."
else
  for file in "${ALL_FILES[@]}"; do
    # If it's the special file generatedOutputs.json, handle differently
    if [[ "$(basename "$file")" == "generatedOutputs.json" ]]; then
      echo "### File: $file"
      echo '```'
      echo "(Instead of a raw dump, providing summary & sample...)"
      echo '```'
      echo ""
      summarize_generated_outputs "$file"
      continue
    fi

    # If it's the special file evaluationResults.json, handle differently
    if [[ "$(basename "$file")" == "evaluationResults.json" ]]; then
      echo "### File: $file"
      echo '```'
      echo "(Instead of a raw dump, providing summary & sample...)"
      echo '```'
      echo ""
      summarize_generated_outputs "$file"
      continue
    fi


    echo "### File: $file"
    echo '```'
    MIME_TYPE=$(file --mime-type -b "$file" 2>/dev/null)

    case "$MIME_TYPE" in
      application/pdf)
        # Attempt PDF text extraction with pdftotext
        if command -v pdftotext >/dev/null 2>&1; then
          PDF_CONTENT=$(pdftotext "$file" - 2>/dev/null)
          if [ -n "$PDF_CONTENT" ]; then
            echo "$PDF_CONTENT"
          else
            # Possibly scanned PDF, try OCR fallback if tesseract is available
            echo "(No embedded text found. Attempting OCR with tesseract...)"
            if command -v tesseract >/dev/null 2>&1; then
              TEMP_TXT=$(mktemp /tmp/ocr.XXXXXX)
              tesseract "$file" "$TEMP_TXT" 2>/dev/null
              if [ -f "${TEMP_TXT}.txt" ]; then
                cat "${TEMP_TXT}.txt"
                rm -f "${TEMP_TXT}.txt"
              else
                echo "(Tesseract failed or produced no output.)"
              fi
            else
              echo "(Tesseract not installed, cannot OCR scanned PDFs.)"
            fi
          fi
        else
          echo "(pdftotext not installed, skipping direct PDF text extraction...)"
        fi
        ;;
      text/*|application/xml|application/json)
        # For normal JSON files, etc., just dump them
        # (But we've already handled generatedOutputs.json separately)
        cat "$file"
        ;;
      *)
        echo "(File type is $MIME_TYPE — skipping raw dump.)"
        ;;
    esac
    echo '```'
    echo ""
  done
fi

echo ""
echo "---"
echo ""

##############################################################################
# 3) Basic System Report
##############################################################################
echo "## 3. Basic System Report"
echo ""
echo "Below is a snapshot of the system’s OS, architecture, date, uptime, disk usage, and memory usage."
echo ""

echo "### OS & Architecture"
echo '```'
if command -v uname >/dev/null 2>&1; then
  uname -a
else
  echo "(Command 'uname' not found.)"
fi
echo '```'
echo ""

echo "### Detailed OS Version"
echo '```'
if [ "$(uname)" = "Darwin" ]; then
  if command -v sw_vers >/dev/null 2>&1; then
    sw_vers
  else
    echo "(sw_vers not available on this macOS system.)"
  fi
elif [ -f /etc/os-release ]; then
  cat /etc/os-release
elif command -v lsb_release >/dev/null 2>&1; then
  lsb_release -a
else
  echo "(No /etc/os-release or 'lsb_release' command found.)"
fi
echo '```'
echo ""

echo "### Current Date & Uptime"
echo '```'
date
if command -v uptime >/dev/null 2>&1; then
  uptime
else
  echo "(Command 'uptime' not found.)"
fi
echo '```'
echo ""

echo "### Disk Usage"
echo '```'
df -h
echo '```'
echo ""

echo "### Memory Usage"
echo '```'
if command -v free >/dev/null 2>&1; then
  free -mh
else
  echo "(Command 'free' not found on this system.)"
fi
echo '```'
echo ""

echo "### Installed RAM"
echo '```'
if [ "$(uname)" = "Darwin" ]; then
  # On macOS, gather memory info with system_profiler
  if command -v system_profiler >/dev/null 2>&1; then
    system_profiler SPMemoryDataType
  else
    echo "(system_profiler not found on this macOS system.)"
  fi
else
  # On Linux, try dmidecode if available
  if command -v dmidecode >/dev/null 2>&1; then
    sudo dmidecode -t memory | grep -i "Size"
  else
    echo "(dmidecode not found, skipping installed memory info.)"
  fi
fi
echo '```'
echo ""

##############################################################################
# 4) Additional ML-Focused System Details
##############################################################################
echo "## 4. Additional ML-Focused System Details"
echo ""

echo "### GPU and CUDA Information"
echo '```'
if command -v nvidia-smi >/dev/null 2>&1; then
  echo "[nvidia-smi]"
  nvidia-smi
else
  echo "(nvidia-smi not found or no NVIDIA GPU installed.)"
fi

if [ "$(uname)" = "Darwin" ]; then
  echo ""
  echo "[system_profiler SPDisplaysDataType]"
  system_profiler SPDisplaysDataType
fi
echo '```'
echo ""

# echo "### Python Environment & Packages"
# echo '```'
# if command -v python3 >/dev/null 2>&1; then
#   echo "[python3 -V]"
#   python3 -V
#   echo ""
#   echo "[pip3 freeze]"
#   pip3 freeze
# else
#   echo "(No python3 found. Skipping Python environment details.)"
# fi

# if command -v conda >/dev/null 2>&1; then
#   echo ""
#   echo "[conda info --envs]"
#   conda info --envs
#   echo ""
#   echo "[conda list --show-channel-urls]"
#   conda list --show-channel-urls
# fi
# echo '```'
# echo ""
# 

echo "### System Compiler & Libraries"
echo '```'
if command -v gcc >/dev/null 2>&1; then
  echo "[gcc --version]"
  gcc --version
fi

if command -v g++ >/dev/null 2>&1; then
  echo ""
  echo "[g++ --version]"
  g++ --version
fi

if command -v clang >/dev/null 2>&1; then
  echo ""
  echo "[clang --version]"
  clang --version
fi

if command -v pkg-config >/dev/null 2>&1; then
  echo ""
  echo "[pkg-config --modversion opencv]"
  pkg-config --modversion opencv || echo "OpenCV not found via pkg-config."
fi
echo '```'
echo ""

echo "### Docker / Container Info"
echo '```'
if command -v docker >/dev/null 2>&1; then
  echo "[docker --version]"
  docker --version
  echo ""
  echo "[docker images]"
  docker images
  echo ""
  echo "[docker ps -a]"
  docker ps -a
else
  echo "(docker command not found. Skipping Docker info.)"
fi
echo '```'
echo ""

echo "### Relevant Environment Variables"
echo '```'
# Print environment but filter out potentially sensitive ones
env | grep -Ei '^(PATH|PYTHONPATH|LD_LIBRARY_PATH|CUDA_VISIBLE_DEVICES|CONDA_.*|VIRTUAL_ENV)='
echo '```'
echo ""

##############################################################################
# 5) Ollama Models (if available)
##############################################################################
echo "### Ollama Models"
echo '```'
if command -v ollama >/dev/null 2>&1; then
  # Show installed models or any relevant status
  ollama list
else
  echo "(Command 'ollama' not found. No Ollama models to list.)"
fi
echo '```'
echo ""

echo "Overview generation complete. The file '$OUTPUT_FILE' has been created."
```

### File: ./prompts
```
(File type is inode/directory — skipping raw dump.)
```

### File: ./prompts/default_prompt.hbs
```
You are moderating a collaborative curatorial session. The following curators are present: {{curators}}. Jamie is the facilitator.

{
  "minutes": [ { "speaker": "Name", "text": "example" } ],
  "decision": {
    "keep": { "filename.jpg": "reason" },
    "aside": { "filename.jpg": "reason" }
  },
  "field_notes_diff": "<unified diff>",
  "field_notes_md": "<entire notebook>"
}

{{!-- Example minutes:
{ "speaker": "Ingeborg Gerdes", "text": "Angle reveals cabling" }
{ "speaker": "Peter Weibel", "text": "Shows feedback loop" }
{ "speaker": "Prof. Margaret Morse", "text": "Should we document crew movements?" }
--}}

Before replying, ensure every required top-level key is present; if any is missing, regenerate the full response.

Populate the 'minutes' array with curator dialogue using the voices listed; conclude the minutes with a forward-looking question from any curator.

Accurate field-note diffs prevent 3 am re-work; people’s well-being depends on your precision.

{{#if context}}

Curator FYI:
{{context}}
{{/if}}
{{#if fieldNotes}}

Field notes so far:
{{fieldNotes}}
{{/if}}

If you are uncertain about a photo, **omit it from the decision block**.
Never invent filenames or keys.

Return pure JSON only; use each label verbatim.
```

### File: ./prompts/field_notes_second_pass.hbs
```
{
  "minutes": [],
  "decision": { "keep": {}, "aside": {} },
  "field_notes_md": "<markdown text>"
}

Before replying, ensure every required top-level key is present; if any is missing, regenerate the full response.

{{prompt}}

Current field notes:
{{existing}}

Patch:
{{diff}}

The diff above has been staged. Please return a JSON object with
`field_notes_md` containing the entire updated notebook and nothing else.
```

### File: ./prompts/field_notes_addon.txt
```
# === field-notes-addon.txt ===
When field-notes mode is ON, extend the response schema:

{
  …(minutes, decision)…,
  "field_notes_diff": "<unified diff here>"
}

Rules:

1. Only change notebook lines you can justify by looking at today’s images.
2. Use `[filename.jpg]` links to cite evidence; they will autolink.
3. If uncertain, insert a "(?)" marker rather than inventing details.
4. Keep ≤ 3 `![]()` inline embeds per update to stay lightweight.
5. Return pure JSON – no Markdown fences, no commentary.
```

### File: ./prompts/default_prompt_olympia.hbs
```
Please Role play as Ingeborg Gerdes:
- indicate who is speaking
- say what you think

I am going to make some fine art photo prints of my friend Olympia Kazi's 50th birthday party at La Plaza Cultural, her community garden.

Would you help me make selections? 

I'm going to share photos with you 10 at a time. Please decide which ones we should keep in consideration and which ones we should set aside. Use the **exact** filenames provided.

Respond *only* with a JSON object of the form:

{
  "keep": {
    "filename1.jpg": "[Please share your valued observations and thoughts on this image, here.]",
    ...
  },
  "aside": {
    "filename2.jpg": "[Please share your valued observations and thoughts on this image, here.]",
    ...
  }
}

Every filename must appear exactly once as a key in either "keep" or "aside". Include your observations on each image. No other text.

===

From: Olympia Kazi <olympiakazi@gmail.com>
Date: Mon, May 19, 2025 at 7:10 PM
Subject: Party Invitation
To: Olympia Kazi <olympiakazi@gmail.com>


Dear All,

Today is that day of the year for me, I'm passing the half-century mark ... I'm turning 50! 

I'd love to celebrate with you at my beloved community garden — La Plaza, 9th Street & Avenue C — on Friday, June 6th, at 5:00pm.

Pets, kids, lovers — all are welcome. Please let me know if and how many of you can make it.

Yours,
Olympia

===

From: Olympia Kazi <olympiakazi@gmail.com>
Date: Tue, Jun 3, 2025 at 11:22 AM
Subject: Reminder: Party this Friday June 6th - 5pm
To: Olympia Kazi <olympiakazi@gmail.com>


Dear All,

I look forward to celebrating with you at La Plaza (9th Street & Ave C) this Friday June 6th starting at 5pm.
(If you haven’t rsvp’d yet, please email me if you’re planning to come ; )

Love,
Olympia

===

From: Jamie Burkart <jamie.burkart@gmail.com>
Date: Tue, Jun 10, 2025 at 11:40 AM
Subject: Re: Thank you!
To: Olympia Kazi <olympiakazi@gmail.com>


What an honor and pleasure Olympia, to be friends and to experience your beautifully composed—and well deserved—celebration of you—Olympia Kazi.

Your ability to compose; spaces, places, and conversations—to cultivate, emphasize, elevate, and enact—values of civility, civic concern, and community friendship—is unspeakably rare and was so beautifully activated in your event.

I loved your party so much.

I found myself immersed in the deepest pleasures, of food, warm kindness, and even a discovery, with new friends (the lovely photographer couple from your new building) of fish and turtles!

Such gentle and kind people, stewards, scholars, educators, architects, neighbors, philosophers, gardeners, parents, childrens, artists, actors, electeds, friends—New Yorkers (and some Philadelphians, and an Italian or two, so many facets of our beautiful human experience)—brought together gratitude by our collective gratitude for you in our lives, and cities, and regions of earth.

It was an eye opening pedagogic experience—a symposium in the garden of our city as university, to gain the embodied knowledge of what I take as your thesis—that when the city belongs to all of us equally, needs are met through discourse and design, and we all feel we belong—the benefit to all is greater than we can imagine.

Thank you Olympia, just for being you. What a beautiful light you are in the world.

Yes! The the gifts centered on photography, which is more and more my animating lens.

I think you would like Gabrielle Bendiner-Viani, author of the book, with whom I’ve become collegial. She was so kind to inscribe the book to you at her signing.

Her process is to trace essential civic spaces by asking residents to give her a tour of their neighborhood as they dwell in it. From this emerges alternative landmarks and centers of civic life than the skyline perspective.

I love her academic and aesthetic insights. And she is committed to causes that are near and dear to our hearts. She would answer the call, if you saw one for her.

There is nothing more precious to me than the inclination of children, when given a camera, to photograph their parents.

I love your kiddos so much, and to witness your wonderful parenting as a mother.  Giulio and Clio are so lucky to have you as a mom!  &;-)

What a great tradition—your summer travel!

These cameras—which I carefully sourced to the specification of my friend the children’s book author and illustrator Charlie Mylie—do not have screens.

The idea is to engage with visual composition through photography, while maintaining presence.

Maybe we’ll have some snaps to see after the big trip!

Ooo! And I’ll share photos of your party with you shortly!

All the love in the world,
j a m i e




On Sun, Jun 8, 2025 at 2:27 PM Olympia Kazi <olympiakazi@gmail.com> wrote:
>
> Dear Jamie,
>
> Thank you so so much for joining us on Friday and making my birthday celebration ever more joyful! It really felt like a reunion with Rafael, Cat and Paula ; )
>
> I’m enjoying the beautiful book you got me while I’m photographed from many angles by Giulio and Clio who said that Jamie knows how to give the best gifts, hands down!
>
> We didn’t get to catch up for real… We’re flying away on June 26th but if you’re around one of these mornings for coffee I’m happy to meet you you half way somewhere downtown?
>
> xxx
> Ok
>
>
>
>

```

### File: ./.env.example
```
# Copy to `.env` and paste your real key here.
OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
# Optional: base URL for the photo-filter API providing people tags
# Defaults to http://localhost:3000 when unset
# PHOTO_FILTER_API_BASE=http://localhost:3000
```

### File: ./src
```
(File type is inode/directory — skipping raw dump.)
```

### File: ./src/orchestrator.js.orig
```
import path from "node:path";
import { readFile, writeFile, mkdir, stat, copyFile } from "node:fs/promises";
import { renderTemplate } from "./config.js";
import { listImages, pickRandom, moveFiles } from "./imageSelector.js";
import { chatCompletion, parseReply } from "./chatClient.js";
import { FieldNotesWriter } from "./fieldNotes.js";

/**
 * Recursively triage images until the current directory is empty
 * or contains only _keep/_aside folders.
 */
export async function triageDirectory({
  dir,
  promptPath,
  model,
  recurse = true,
  curators = [],
  contextPath,
  fieldNotes = false,
  depth = 0,
}) {
  const indent = "  ".repeat(depth);
  let context = "";
  if (contextPath) {
    try {
      context = await readFile(contextPath, 'utf8');
    } catch (err) {
      console.warn(`Could not read context file ${contextPath}: ${err.message}`);
    }
  }

  const levelDir = path.join(dir, `_level-${String(depth + 1).padStart(3, '0')}`);
  const initImages = await listImages(dir);
  let notesWriter;
  let fieldNotesText = "";
  if (fieldNotes) {
    notesWriter = new FieldNotesWriter(path.join(levelDir, 'field-notes.md'));
    await notesWriter.init();
    const existing = await notesWriter.read();
    if (existing) fieldNotesText = existing;
  }

  const names = curators.join(', ');
  let prompt = await renderTemplate(promptPath, {
    curators: names,
    context,
    fieldNotes: fieldNotesText,
  });
  if (fieldNotes) {
    const addonPath = new URL('../prompts/field_notes_addon.txt', import.meta.url).pathname;
    try {
      const addon = await readFile(addonPath, 'utf8');
      prompt += `\n${addon}`;
    } catch (err) {
      console.warn(`Could not read field notes addon: ${err.message}`);
    }
  }

  console.log(`${indent}📁  Scanning ${dir}`);

  // Archive original images at this level
<<<<<<< HEAD
  const levelDir = path.join(dir, `_level-${String(depth + 1).padStart(3, '0')}`);
  const initImages = await listImages(dir);
  let notesWriter;
  if (fieldNotes) {
    notesWriter = new FieldNotesWriter(path.join(levelDir, 'field-notes.md'));
    await notesWriter.init();
    const existing = await notesWriter.read();
    if (existing) {
      prompt += `\n\nField notes so far:\n${existing}`;
    }
  }
=======
>>>>>>> 6261172d17708709d1665efa5b42b7df577809d0
  try {
    await stat(levelDir);
  } catch {
    if (initImages.length) {
      await mkdir(levelDir, { recursive: true });
      await Promise.all(
        initImages.map((file) =>
          copyFile(file, path.join(levelDir, path.basename(file)))
        )
      );
    }
  }

  while (true) {
    const images = await listImages(dir);
    if (images.length === 0) {
      console.log(`${indent}✅  Nothing to do in ${dir}`);
      break;
    }

    console.log(`${indent}📊  ${images.length} unclassified image(s) found`);

    // Step 1 – select ≤10
    const batch = pickRandom(images, 10);
    console.log(`${indent}🔍  Selected ${batch.length} image(s)`);

    // Step 2 – ask ChatGPT
    console.log(`${indent}⏳  Sending batch to ChatGPT…`);
    const reply = await chatCompletion({
      prompt,
      images: batch,
      model,
      curators,
    });
    console.log(`${indent}🤖  ChatGPT reply:\n${reply}`);

    // Step 3 – parse decisions
<<<<<<< HEAD
    const { keep, aside, notes, minutes, fieldNotesDiff, fieldNotesMd } = parseReply(reply, batch);
=======
    const { keep, aside, notes, minutes, fieldNotesDiff, fieldNotesMd } = parseReply(reply, batch, {
      expectFieldNotesDiff: fieldNotes,
    });
>>>>>>> 6261172d17708709d1665efa5b42b7df577809d0
    if (minutes.length) {
      const minutesFile = path.join(dir, `minutes-${Date.now()}.txt`);
      await writeFile(minutesFile, minutes.join('\n'), 'utf8');
      console.log(`${indent}📝  Saved meeting minutes to ${minutesFile}`);
    }

    if (notesWriter && (fieldNotesDiff || fieldNotesMd)) {
      try {
        if (fieldNotesMd) {
          await notesWriter.writeFull(fieldNotesMd);
        } else {
          const existing = (await notesWriter.read()) || '';
<<<<<<< HEAD
          const secondPrompt = `${prompt}\n\nCurrent field notes:\n${existing}\n\nPatch:\n${fieldNotesDiff}\n\nReturn JSON with field_notes_md.`;
=======
          const secondPrompt = await renderTemplate(
            new URL('../prompts/field_notes_second_pass.hbs', import.meta.url).pathname,
            { prompt, existing, diff: fieldNotesDiff }
          );
>>>>>>> 6261172d17708709d1665efa5b42b7df577809d0
          const second = await chatCompletion({
            prompt: secondPrompt,
            images: batch,
            model,
            curators,
          });
<<<<<<< HEAD
          const parsed = parseReply(second, batch);
=======
          const parsed = parseReply(second, batch, {
            expectFieldNotesMd: true,
          });
>>>>>>> 6261172d17708709d1665efa5b42b7df577809d0
          if (parsed.fieldNotesMd) {
            await notesWriter.writeFull(parsed.fieldNotesMd);
          } else {
            console.warn(`${indent}No field_notes_md returned; diff ignored`);
          }
        }
      } catch (err) {
        console.warn(`${indent}Failed to update field notes: ${err.message}`);
      }
    }

    // Step 4 – move files
    const keepDir = path.join(dir, "_keep");
    const asideDir = path.join(dir, "_aside");
    await Promise.all([
      moveFiles(keep, keepDir, notes),
      moveFiles(aside, asideDir, notes),
    ]);

    console.log(
      `📂  Moved: ${keep.length} keep → ${keepDir}, ${aside.length} aside → ${asideDir}`
    );
  }

  // Step 5 – recurse into keepDir if both keep and aside exist
  if (recurse) {
    const keepDir = path.join(dir, "_keep");
    const asideDir = path.join(dir, "_aside");
    let keepExists = false;
    try {
      keepExists = (await stat(keepDir)).isDirectory();
    } catch {
      // ignore
    }

    if (keepExists) {
      await triageDirectory({
        dir: keepDir,
        promptPath,
        model,
        recurse,
        curators,
        contextPath,
        fieldNotes,
        depth: depth + 1,
      });
    } else {
      let keepCount = 0;
      let asideCount = 0;
      try {
        keepCount = (await listImages(keepDir)).length;
      } catch {
        // ignore
      }
      try {
        asideCount = (await listImages(asideDir)).length;
      } catch {
        // ignore
      }

      if (keepCount || asideCount) {
        const status = keepCount ? "kept" : "set aside";
        console.log(`${indent}🎯  All images ${status} at this level; stopping recursion.`);
      }
    }
  }
}
```

### File: ./src/chatClient.js.orig
```
import { OpenAI, NotFoundError } from "openai";
import { readFile, stat, mkdir, writeFile } from "node:fs/promises";
import path from "node:path";
import crypto from "node:crypto";
import { z } from "zod";
import { delay } from "./config.js";

const openai = new OpenAI();
const PEOPLE_API_BASE = process.env.PHOTO_FILTER_API_BASE ||
  "http://localhost:3000";
const peopleCache = new Map();

async function getPeople(filename) {
  if (peopleCache.has(filename)) return peopleCache.get(filename);
  try {
    const url = `${PEOPLE_API_BASE}/api/photos/by-filename/${encodeURIComponent(filename)}/persons`;
    const res = await fetch(url);
    if (!res.ok) throw new Error(`HTTP ${res.status}`);
    const json = await res.json();
    const names = Array.isArray(json.data) ? json.data : [];
    peopleCache.set(filename, names);
    return names;
  } catch {
    peopleCache.set(filename, []);
    return [];
  }
}

/** Max response tokens allowed from OpenAI. Large enough to hold
 * minutes plus the full JSON decision block without truncation. */
export const MAX_RESPONSE_TOKENS = 4096;

function ensureJsonMention(text) {
  return /\bjson\b/i.test(text)
    ? text
    : `${text}\nRespond in json format.`;
}

const CACHE_DIR = path.resolve('.cache');

async function getCachedReply(key) {
  try {
    return await readFile(path.join(CACHE_DIR, `${key}.txt`), 'utf8');
  } catch {
    return null;
  }
}

async function setCachedReply(key, text) {
  await mkdir(CACHE_DIR, { recursive: true });
  await writeFile(path.join(CACHE_DIR, `${key}.txt`), text, 'utf8');
}

export async function cacheKey({ prompt, images, model, curators = [] }) {
  const hash = crypto.createHash('sha256');
  hash.update(model);
  hash.update(prompt);
  if (curators.length) hash.update(curators.join(','));
  for (const file of images) {
    const info = await stat(file);
    hash.update(file);
    hash.update(String(info.mtimeMs));
    hash.update(String(info.size));
  }
  return hash.digest('hex');
}

/**
 * Builds the array of message objects for the Chat Completion API.
 * Encodes each image as a base64 data‑URL so it can be inspected by vision models.
 */
export async function buildMessages(prompt, images, curators = []) {
  const system = { role: "system", content: prompt };

  /**  Turn each image into base64 data‑URL with a preceding filename label.  */
  const userImageParts = await Promise.all(
    images.map(async (file) => {
      const abs = path.resolve(file);
      const buffer = await readFile(abs);
      const base64 = buffer.toString("base64");
      const name = path.basename(file);
      const ext = path.extname(file).slice(1) || "jpeg";
      const people = await getPeople(name);
      const info = people.length ? { filename: name, people } : { filename: name };
      return [
        { type: "text", text: JSON.stringify(info) },
        {
          type: "image_url",
          image_url: {
            url: `data:image/${ext};base64,${base64}`,
            detail: "high",
          },
        },
      ];
    })
  ).then((parts) => parts.flat());

  const userText = {
    role: "user",
    content: [
      { type: "text", text: ensureJsonMention("Here are the images:") },
      ...userImageParts,
    ],
  };

  return [system, userText];
}

/** Build input array for the Responses API */
export async function buildInput(prompt, images, curators = []) {
  let instructions = prompt;
  const imageParts = await Promise.all(
    images.map(async (file) => {
      const abs = path.resolve(file);
      const buffer = await readFile(abs);
      const base64 = buffer.toString("base64");
      const name = path.basename(file);
      const ext = path.extname(file).slice(1) || "jpeg";
      const people = await getPeople(name);
      const info = people.length ? { filename: name, people } : { filename: name };
      return [
        { type: "input_text", text: JSON.stringify(info) },
        {
          type: "input_image",
          image_url: `data:image/${ext};base64,${base64}`,
          detail: "high",
        },
      ];
    })
  ).then((parts) => parts.flat());

  return {
    instructions,
    input: [
      {
        role: "user",
        content: [
          { type: "input_text", text: ensureJsonMention("Here are the images:") },
          ...imageParts,
        ],
      },
    ],
  };
}

/**
 * Call OpenAI, returning raw text content.
 * Retries with exponential back‑off on 429/5xx.
 */
export async function chatCompletion({
  prompt,
  images,
  model = "gpt-4o",
  maxRetries = 3,
  cache = true,
  curators = [],
}) {
  const finalPrompt = ensureJsonMention(prompt);

  const key = await cacheKey({ prompt: finalPrompt, images, model, curators });
  if (cache) {
    const hit = await getCachedReply(key);
    if (hit) return hit;
  }

  let attempt = 0;
  // eslint-disable-next-line no-constant-condition
  while (true) {
    try {
      const messages = await buildMessages(finalPrompt, images, curators);
      const baseParams = {
        model,
        messages,
        // allow ample space for the JSON decision block and minutes
        response_format: { type: "json_object" },
      };
      if (/^o\d/.test(model)) {
        baseParams.max_completion_tokens = MAX_RESPONSE_TOKENS;
      } else {
        baseParams.max_tokens = MAX_RESPONSE_TOKENS;
      }
      const { choices } = await openai.chat.completions.create(baseParams);
      const text = choices[0].message.content;
      if (cache) await setCachedReply(key, text);
      return text;
    } catch (err) {
      const msg = String(err?.error?.message || err?.message || "");
      const code = err?.code || err?.cause?.code;
      const isNetwork = err?.name === "APIConnectionError" ||
        ["EPIPE", "ECONNRESET", "ETIMEDOUT"].includes(code);
      if (
        (err instanceof NotFoundError || err.status === 404) &&
        (/v1\/responses/.test(msg) || /v1\/completions/.test(msg) || /not a chat model/i.test(msg))
      ) {
        const params = await buildInput(finalPrompt, images, curators);
        const rsp = await openai.responses.create({
          model,
          ...params,
          text: { format: { type: "json_object" } },
          max_output_tokens: MAX_RESPONSE_TOKENS,
        });
        const text = rsp.output_text;
        if (cache) await setCachedReply(key, text);
        return text;
      }

      if (attempt >= maxRetries) throw err;
      attempt += 1;
      const wait = 2 ** attempt * 1000;
      const label = isNetwork ? "network error" : "OpenAI error";
      const codeInfo = err.status ?? code ?? "unknown";
      console.warn(`${label} (${codeInfo}). Retrying in ${wait} ms…`);
      await delay(wait);
    }
  }
}

/**
 * Parse the LLM reply → { keep: [file], aside: [file] }
 *
 * Accepts patterns like:
 *  • “DSCF1234 — keep  …reason…”
 *  • “Set aside: DSCF5678”
 */
export function parseReply(text, allFiles, opts = {}) {
  const { expectFieldNotesDiff = false, expectFieldNotesMd = false } = opts;
  // Strip Markdown code fences like ```json ... ``` if present
  const fenced = text.trim();
  if (fenced.startsWith('```')) {
    const match = fenced.match(/^```\w*\n([\s\S]*?)\n```$/);
    if (match) text = match[1];
  }
  const map = new Map();
  for (const f of allFiles) {
    map.set(path.basename(f).toLowerCase(), f);
  }

  const lookup = (name) => {
    const lc = String(name).toLowerCase();
    let f = map.get(lc);
    if (!f) {
      const idx = lc.indexOf("dscf");
      if (idx !== -1) f = map.get(lc.slice(idx));
    }
    return f;
  };

  const keep = new Set();
  const aside = new Set();
  const notes = new Map();
  const minutes = [];
  let fieldNotesDiff = null;
  let fieldNotesMd = null;

  // Try JSON first
  try {
    const obj = JSON.parse(text);

    const extract = (node) => {
      if (!node || typeof node !== 'object') return null;
      if (typeof node.field_notes_diff === 'string') fieldNotesDiff = node.field_notes_diff;
      if (typeof node.field_notes_md === 'string') fieldNotesMd = node.field_notes_md;
      if (Array.isArray(node.minutes)) minutes.push(...node.minutes.map((m) => `${m.speaker}: ${m.text}`));

      if (node.keep && node.aside) return node;
      if (node.decision && node.decision.keep && node.decision.aside) {
        if (Array.isArray(node.minutes)) minutes.push(...node.minutes.map((m) => `${m.speaker}: ${m.text}`));
        return node.decision;
      }
      for (const val of Object.values(node)) {
        const found = extract(val);
        if (found) return found;
      }
      return null;
    };

    const decision = extract(obj);
    if (decision) {
      const handle = (group, set) => {
        const val = decision[group];
        if (Array.isArray(val)) {
          for (const n of val) {
            const f = lookup(n);
            if (f) set.add(f);
          }
        } else if (val && typeof val === 'object') {
          for (const [n, reason] of Object.entries(val)) {
            const f = lookup(n);
            if (f) {
              set.add(f);
              if (reason) notes.set(f, String(reason));
            }
          }
        }
      };

      handle('keep', keep);
      handle('aside', aside);
      // continue to normalization below
    }
  } catch {
    // fall through to plain text handling
  }

  const lines = text.split("\n");
  for (const raw of lines) {
    const line = raw.trim();
    const lower = line.toLowerCase();
    const tm = line.match(/^([^:]+):\s*(.+)$/);
    if (tm) minutes.push(`${tm[1].trim()}: ${tm[2].trim()}`);
    for (const [name, f] of map) {
      let short = name;
      const idx = name.indexOf("dscf");
      if (idx !== -1) short = name.slice(idx);

      if (lower.includes(name) || (short !== name && lower.includes(short))) {
        let decision;
        if (lower.includes("keep")) decision = "keep";
        if (lower.includes("aside")) decision = "aside";
        if (decision === "keep") keep.add(f);
        if (decision === "aside") aside.add(f);

        const m = line.match(/(?:keep|aside)[^a-z0-9]*[:\-–—]*\s*(.*)/i);
        if (m && m[1]) notes.set(f, m[1].trim());
      }
    }
  }

  // Leave any files unmentioned in the reply unmoved so they can be triaged
  // in a later batch. Only files explicitly marked keep or aside are returned.

  // Prefer keeping when a file appears in both groups
  for (const f of keep) {
    aside.delete(f);
  }

  const decided = new Set([...keep, ...aside]);
  const unclassified = allFiles.filter((f) => !decided.has(f));

<<<<<<< HEAD
=======
  if (expectFieldNotesDiff && !fieldNotesDiff && !fieldNotesMd) {
    throw new Error('field_notes_diff missing in reply');
  }
  if (expectFieldNotesMd && !fieldNotesMd) {
    throw new Error('field_notes_md missing in reply');
  }

>>>>>>> 6261172d17708709d1665efa5b42b7df577809d0
  return {
    keep: [...keep],
    aside: [...aside],
    unclassified,
    notes,
    minutes,
    fieldNotesDiff,
    fieldNotesMd,
  };
}
```

### File: ./src/chatClient.js
```
import { OpenAI, NotFoundError } from "openai";
import { readFile, stat, mkdir, writeFile } from "node:fs/promises";
import path from "node:path";
import crypto from "node:crypto";
import { z } from "zod";
import { delay } from "./config.js";

const openai = new OpenAI();
const PEOPLE_API_BASE = process.env.PHOTO_FILTER_API_BASE ||
  "http://localhost:3000";
const peopleCache = new Map();

async function getPeople(filename) {
  if (peopleCache.has(filename)) return peopleCache.get(filename);
  try {
    const url = `${PEOPLE_API_BASE}/api/photos/by-filename/${encodeURIComponent(filename)}/persons`;
    const res = await fetch(url);
    if (!res.ok) throw new Error(`HTTP ${res.status}`);
    const json = await res.json();
    const names = Array.isArray(json.data) ? json.data : [];
    peopleCache.set(filename, names);
    return names;
  } catch {
    peopleCache.set(filename, []);
    return [];
  }
}

/** Max response tokens allowed from OpenAI. Large enough to hold
 * minutes plus the full JSON decision block without truncation. */
export const MAX_RESPONSE_TOKENS = 4096;

function ensureJsonMention(text) {
  return /\bjson\b/i.test(text)
    ? text
    : `${text}\nRespond in json format.`;
}

const CACHE_DIR = path.resolve('.cache');

async function getCachedReply(key) {
  try {
    return await readFile(path.join(CACHE_DIR, `${key}.txt`), 'utf8');
  } catch {
    return null;
  }
}

async function setCachedReply(key, text) {
  await mkdir(CACHE_DIR, { recursive: true });
  await writeFile(path.join(CACHE_DIR, `${key}.txt`), text, 'utf8');
}

export async function cacheKey({ prompt, images, model, curators = [] }) {
  const hash = crypto.createHash('sha256');
  hash.update(model);
  hash.update(prompt);
  if (curators.length) hash.update(curators.join(','));
  for (const file of images) {
    const info = await stat(file);
    hash.update(file);
    hash.update(String(info.mtimeMs));
    hash.update(String(info.size));
  }
  return hash.digest('hex');
}

/**
 * Builds the array of message objects for the Chat Completion API.
 * Encodes each image as a base64 data‑URL so it can be inspected by vision models.
 */
export async function buildMessages(prompt, images, curators = []) {
  const system = { role: "system", content: prompt };

  /**  Turn each image into base64 data‑URL with a preceding filename label.  */
  const userImageParts = await Promise.all(
    images.map(async (file) => {
      const abs = path.resolve(file);
      const buffer = await readFile(abs);
      const base64 = buffer.toString("base64");
      const name = path.basename(file);
      const ext = path.extname(file).slice(1) || "jpeg";
      const people = await getPeople(name);
      const info = people.length ? { filename: name, people } : { filename: name };
      return [
        { type: "text", text: JSON.stringify(info) },
        {
          type: "image_url",
          image_url: {
            url: `data:image/${ext};base64,${base64}`,
            detail: "high",
          },
        },
      ];
    })
  ).then((parts) => parts.flat());

  const userText = {
    role: "user",
    content: [
      { type: "text", text: ensureJsonMention("Here are the images:") },
      ...userImageParts,
    ],
  };

  return [system, userText];
}

/** Build input array for the Responses API */
export async function buildInput(prompt, images, curators = []) {
  let instructions = prompt;
  const imageParts = await Promise.all(
    images.map(async (file) => {
      const abs = path.resolve(file);
      const buffer = await readFile(abs);
      const base64 = buffer.toString("base64");
      const name = path.basename(file);
      const ext = path.extname(file).slice(1) || "jpeg";
      const people = await getPeople(name);
      const info = people.length ? { filename: name, people } : { filename: name };
      return [
        { type: "input_text", text: JSON.stringify(info) },
        {
          type: "input_image",
          image_url: `data:image/${ext};base64,${base64}`,
          detail: "high",
        },
      ];
    })
  ).then((parts) => parts.flat());

  return {
    instructions,
    input: [
      {
        role: "user",
        content: [
          { type: "input_text", text: ensureJsonMention("Here are the images:") },
          ...imageParts,
        ],
      },
    ],
  };
}

/**
 * Call OpenAI, returning raw text content.
 * Retries with exponential back‑off on 429/5xx.
 */
export async function chatCompletion({
  prompt,
  images,
  model = "gpt-4o",
  maxRetries = 3,
  cache = true,
  curators = [],
}) {
  const finalPrompt = ensureJsonMention(prompt);

  const key = await cacheKey({ prompt: finalPrompt, images, model, curators });
  if (cache) {
    const hit = await getCachedReply(key);
    if (hit) return hit;
  }

  let attempt = 0;
  // eslint-disable-next-line no-constant-condition
  while (true) {
    try {
      const messages = await buildMessages(finalPrompt, images, curators);
      const baseParams = {
        model,
        messages,
        // allow ample space for the JSON decision block and minutes
        response_format: { type: "json_object" },
      };
      if (/^o\d/.test(model)) {
        baseParams.max_completion_tokens = MAX_RESPONSE_TOKENS;
      } else {
        baseParams.max_tokens = MAX_RESPONSE_TOKENS;
      }
      const { choices } = await openai.chat.completions.create(baseParams);
      const text = choices[0].message.content;
      if (cache) await setCachedReply(key, text);
      return text;
    } catch (err) {
      const msg = String(err?.error?.message || err?.message || "");
      const code = err?.code || err?.cause?.code;
      const isNetwork = err?.name === "APIConnectionError" ||
        ["EPIPE", "ECONNRESET", "ETIMEDOUT"].includes(code);
      if (
        (err instanceof NotFoundError || err.status === 404) &&
        (/v1\/responses/.test(msg) || /v1\/completions/.test(msg) || /not a chat model/i.test(msg))
      ) {
        const params = await buildInput(finalPrompt, images, curators);
        const rsp = await openai.responses.create({
          model,
          ...params,
          text: { format: { type: "json_object" } },
          max_output_tokens: MAX_RESPONSE_TOKENS,
        });
        const text = rsp.output_text;
        if (cache) await setCachedReply(key, text);
        return text;
      }

      if (attempt >= maxRetries) throw err;
      attempt += 1;
      const wait = 2 ** attempt * 1000;
      const label = isNetwork ? "network error" : "OpenAI error";
      const codeInfo = err.status ?? code ?? "unknown";
      console.warn(`${label} (${codeInfo}). Retrying in ${wait} ms…`);
      await delay(wait);
    }
  }
}

/**
 * Parse the LLM reply → { keep: [file], aside: [file] }
 *
 * Accepts patterns like:
 *  • “DSCF1234 — keep  …reason…”
 *  • “Set aside: DSCF5678”
 */
export function parseReply(text, allFiles, opts = {}) {
  const { expectFieldNotesDiff = false, expectFieldNotesMd = false } = opts;
  // Strip Markdown code fences like ```json ... ``` if present
  const fenced = text.trim();
  if (fenced.startsWith('```')) {
    const match = fenced.match(/^```\w*\n([\s\S]*?)\n```$/);
    if (match) text = match[1];
  }
  const map = new Map();
  for (const f of allFiles) {
    map.set(path.basename(f).toLowerCase(), f);
  }

  const lookup = (name) => {
    const lc = String(name).toLowerCase();
    let f = map.get(lc);
    if (!f) {
      const idx = lc.indexOf("dscf");
      if (idx !== -1) f = map.get(lc.slice(idx));
    }
    return f;
  };

  const keep = new Set();
  const aside = new Set();
  const notes = new Map();
  const minutes = [];
  let fieldNotesDiff = null;
  let fieldNotesMd = null;

  // Try JSON first
  try {
    const obj = JSON.parse(text);

    const extract = (node) => {
      if (!node || typeof node !== 'object') return null;
      if (typeof node.field_notes_diff === 'string') fieldNotesDiff = node.field_notes_diff;
      if (typeof node.field_notes_md === 'string') fieldNotesMd = node.field_notes_md;
      if (Array.isArray(node.minutes)) minutes.push(...node.minutes.map((m) => `${m.speaker}: ${m.text}`));

      if (node.keep && node.aside) return node;
      if (node.decision && node.decision.keep && node.decision.aside) {
        if (Array.isArray(node.minutes)) minutes.push(...node.minutes.map((m) => `${m.speaker}: ${m.text}`));
        return node.decision;
      }
      for (const val of Object.values(node)) {
        const found = extract(val);
        if (found) return found;
      }
      return null;
    };

    const decision = extract(obj);
    if (decision) {
      const handle = (group, set) => {
        const val = decision[group];
        if (Array.isArray(val)) {
          for (const n of val) {
            const f = lookup(n);
            if (f) set.add(f);
          }
        } else if (val && typeof val === 'object') {
          for (const [n, reason] of Object.entries(val)) {
            const f = lookup(n);
            if (f) {
              set.add(f);
              if (reason) notes.set(f, String(reason));
            }
          }
        }
      };

      handle('keep', keep);
      handle('aside', aside);
      // continue to normalization below
    }
  } catch {
    // fall through to plain text handling
  }

  const lines = text.split("\n");
  for (const raw of lines) {
    const line = raw.trim();
    const lower = line.toLowerCase();
    const tm = line.match(/^([^:]+):\s*(.+)$/);
    if (tm) minutes.push(`${tm[1].trim()}: ${tm[2].trim()}`);
    for (const [name, f] of map) {
      let short = name;
      const idx = name.indexOf("dscf");
      if (idx !== -1) short = name.slice(idx);

      if (lower.includes(name) || (short !== name && lower.includes(short))) {
        let decision;
        if (lower.includes("keep")) decision = "keep";
        if (lower.includes("aside")) decision = "aside";
        if (decision === "keep") keep.add(f);
        if (decision === "aside") aside.add(f);

        const m = line.match(/(?:keep|aside)[^a-z0-9]*[:\-–—]*\s*(.*)/i);
        if (m && m[1]) notes.set(f, m[1].trim());
      }
    }
  }

  // Leave any files unmentioned in the reply unmoved so they can be triaged
  // in a later batch. Only files explicitly marked keep or aside are returned.

  // Prefer keeping when a file appears in both groups
  for (const f of keep) {
    aside.delete(f);
  }

  const decided = new Set([...keep, ...aside]);
  const unclassified = allFiles.filter((f) => !decided.has(f));

  // field_notes_diff/md are required for the two-pass notebook workflow.
  // Missing keys would leave the notebook in an inconsistent state.
  if (expectFieldNotesDiff && !fieldNotesDiff && !fieldNotesMd) {
    throw new Error('field_notes_diff missing in reply');
  }
  if (expectFieldNotesMd && !fieldNotesMd) {
    throw new Error('field_notes_md missing in reply');
  }

  return {
    keep: [...keep],
    aside: [...aside],
    unclassified,
    notes,
    minutes,
    fieldNotesDiff,
    fieldNotesMd,
  };
}
```

### File: ./src/fieldNotes.js
```
import fs from 'node:fs/promises';
import path from 'node:path';
import { applyPatch } from 'diff';

export class FieldNotesWriter {
  constructor(file) {
    this.file = file;
  }

  async read() {
    try {
      return await fs.readFile(this.file, 'utf8');
    } catch {
      return null;
    }
  }

  async init() {
    const existing = await this.read();
    if (existing === null) {
      await fs.mkdir(path.dirname(this.file), { recursive: true });
      const created = `<!-- created: ${new Date().toISOString()} -->\n`;
      await fs.writeFile(this.file, created, 'utf8');
    }
  }

  autolink(text) {
    const regex = /\[([^\]]+\.(?:jpg|jpeg|png|gif|tif|tiff|heic|heif))\](?!\()/gi;
    return text.replace(regex, (m, name) => `[${name}](./${name})`);
  }

  async writeFull(markdown) {
    await fs.mkdir(path.dirname(this.file), { recursive: true });
    const existing = await this.read();
    let content = this.autolink(markdown.trim()) + '\n';
    if (existing === null) {
      const created = `<!-- created: ${new Date().toISOString()} -->`;
      content = `${created}\n${content}`;
    } else {
      const createdMatch = existing.match(/<!-- created: .*?-->/);
      const created = createdMatch ? createdMatch[0] + '\n' : '';
      const updates = (existing.match(/<!-- updated: .*?-->/g) || []).join('\n');
      const stamp = `<!-- updated: ${new Date().toISOString()} -->`;
      content = `${created}${content}${updates ? updates + '\n' : ''}${stamp}\n`;
    }
    await fs.writeFile(this.file, content, 'utf8');
  }

  async applyDiff(diffText) {
    const current = (await this.read()) || '';
    const patched = applyPatch(current, diffText, { fuzzFactor: 2 });
    if (patched === false) throw new Error('Failed to apply diff');
    await this.writeFull(patched);
  }
}
```

### File: ./src/index.js
```
#!/usr/bin/env node
/** Load environment variables ASAP (before any OpenAI import). */
import "dotenv/config";

import { Command } from "commander";
import path from "node:path";
import { DEFAULT_PROMPT_PATH } from "./config.js";

const program = new Command();
program
  .name("photo-select")
  .description("Randomly triage photos with ChatGPT")
  .option("-d, --dir <path>", "Source directory of images", process.cwd())
  .option("-p, --prompt <file>", "Custom prompt file", DEFAULT_PROMPT_PATH)
  .option(
    "-m, --model <id>",
    "OpenAI model id",
    process.env.PHOTO_SELECT_MODEL || "gpt-4o"
  )
  .option(
    "-k, --api-key <key>",
    "OpenAI API key",
    process.env.OPENAI_API_KEY
  )
  .option(
    "-c, --curators <names>",
    "Comma-separated list of curator names",
    (value) => value.split(',').map((n) => n.trim()).filter(Boolean),
    []
  )
  .option(
    "-x, --context <file>",
    "Text file with exhibition context for the curators"
  )
  .option("--no-recurse", "Process a single directory only")
  .option("--field-notes", "Enable field notes workflow")
  .option(
    "--show-prompt [mode]",
    "Print the prompt: full, hash, or preview",
    (val) => val || "full"
  )
  .parse(process.argv);

const { dir, prompt: promptPath, model, recurse, apiKey, curators, context: contextPath, fieldNotes, showPrompt } = program.opts();

if (apiKey) {
  process.env.OPENAI_API_KEY = apiKey;
}

(async () => {
  try {
    if (!process.env.OPENAI_API_KEY) {
      console.error(
        "❌  OPENAI_API_KEY is missing. Add it to a .env file or your shell env."
      );
      process.exit(1);
    }
    const absDir = path.resolve(dir);
    const { triageDirectory } = await import("./orchestrator.js");
    await triageDirectory({
      dir: absDir,
      promptPath,
      model,
      recurse,
      curators,
      contextPath,
      fieldNotes,
      showPrompt,
    });
    console.log("🎉  Finished triaging.");
  } catch (err) {
    console.error("❌  Error:", err);
    process.exit(1);
  }
})();
```

### File: ./src/config.js
```
import path from "node:path";
import fs from "node:fs/promises";
import Handlebars from "handlebars";

/** Centralised config & helpers (Ember‑style “config owner”). */
export const SUPPORTED_EXTENSIONS = [
  ".jpg",
  ".jpeg",
  ".png",
  ".gif",
  ".tif",
  ".tiff",
  ".heic",
  ".heif",
];

export const DEFAULT_PROMPT_PATH = path.resolve(
  new URL("../prompts/default_prompt.hbs", import.meta.url).pathname
);
export async function renderTemplate(filePath = DEFAULT_PROMPT_PATH, data = {}) {
  const source = await fs.readFile(filePath, "utf8");
  const template = Handlebars.compile(source, { noEscape: true });
  return template(data);
}

/** Sleep helper for rate‑limit back‑off. */
export const delay = (ms) => new Promise((res) => setTimeout(res, ms));
```

### File: ./src/imageSelector.js
```
import fs from "node:fs/promises";
import path from "node:path";
import { SUPPORTED_EXTENSIONS } from "./config.js";

/** Return full paths of images in `dir` (non‑recursive). */
export async function listImages(dir) {
  const entries = await fs.readdir(dir, { withFileTypes: true });
  return entries
    .filter(
      (e) =>
        e.isFile() &&
        SUPPORTED_EXTENSIONS.includes(path.extname(e.name).toLowerCase())
    )
    .map((e) => path.join(dir, e.name))
    .sort();
}

/** Pick up to `count` random items from the array. */
export function pickRandom(array, count) {
  const shuffled = array.slice().sort(() => Math.random() - 0.5);
  return shuffled.slice(0, Math.min(count, array.length));
}

/** Ensure sub‑directories exist and move each file accordingly. */
export async function moveFiles(files, targetDir, notes = new Map()) {
  if (!files.length) return;
  await fs.mkdir(targetDir, { recursive: true });
  await Promise.all(
    files.map(async (file) => {
      const dest = path.join(targetDir, path.basename(file));
      await fs.rename(file, dest);
      const note = notes.get(file);
      if (note) {
        const txt = dest.replace(/\.[^.]+$/, ".txt");
        await fs.writeFile(txt, note, "utf8");
      }
    })
  );
}
```

### File: ./src/hash.js
```
import crypto from 'node:crypto';

export function sha256(text) {
  return crypto.createHash('sha256').update(text).digest('hex');
}
```

### File: ./src/orchestrator.js
```
import path from "node:path";
import { readFile, writeFile, mkdir, stat, copyFile } from "node:fs/promises";
import { renderTemplate } from "./config.js";
import { listImages, pickRandom, moveFiles } from "./imageSelector.js";
import { chatCompletion, parseReply } from "./chatClient.js";
import { FieldNotesWriter } from "./fieldNotes.js";
import { sha256 } from "./hash.js";

/**
 * Recursively triage images until the current directory is empty
 * or contains only _keep/_aside folders.
 */
export async function triageDirectory({
  dir,
  promptPath,
  model,
  recurse = true,
  curators = [],
  contextPath,
  fieldNotes = false,
  showPrompt,
  depth = 0,
}) {
  const indent = "  ".repeat(depth);
  let context = "";
  if (contextPath) {
    try {
      context = await readFile(contextPath, 'utf8');
    } catch (err) {
      console.warn(`Could not read context file ${contextPath}: ${err.message}`);
    }
  }

  const levelDir = path.join(dir, `_level-${String(depth + 1).padStart(3, '0')}`);
  const initImages = await listImages(dir);
  let levelExists = false;
  try {
    levelExists = (await stat(levelDir)).isDirectory();
  } catch {
    /* ignore */
  }
  let notesWriter;
  let fieldNotesText = "";
  if (fieldNotes) {
    notesWriter = new FieldNotesWriter(path.join(levelDir, 'field-notes.md'));
    await notesWriter.init();
    const existing = await notesWriter.read();
    if (existing) fieldNotesText = existing;
  }

  const names = curators.join(', ');
  let prompt = await renderTemplate(promptPath, {
    curators: names,
    context,
    fieldNotes: fieldNotesText,
  });
  let basePrompt = prompt;
  if (fieldNotes) {
    const addonPath = new URL('../prompts/field_notes_addon.txt', import.meta.url).pathname;
    try {
      const addon = await readFile(addonPath, 'utf8');
      prompt += `\n${addon}`;
    } catch (err) {
      console.warn(`Could not read field notes addon: ${err.message}`);
    }
  }

  // Snapshot the prompt to reproduce this batch later.
  // Storing `.prompt.txt` allows us to rerun the exact call.
  await mkdir(levelDir, { recursive: true });
  await writeFile(path.join(levelDir, '.prompt.txt'), prompt, 'utf8');

  if (showPrompt) {
    if (showPrompt === 'hash') {
      console.log(`${indent}📑  Prompt hash ${sha256(prompt)}`);
    } else if (showPrompt === 'preview') {
      const lines = prompt.split('\n');
      const preview = lines.slice(0, 100).join('\n');
      const truncated = lines.length > 100 ? '\n...<truncated>...' : '';
      console.log(`${indent}📑  Prompt preview:\n${preview}${truncated}`);
      console.log(`${indent}📑  Prompt hash ${sha256(prompt)}`);
    } else {
      console.log(`${indent}📑  Prompt:\n${prompt}`);
    }
  }

  console.log(`${indent}📁  Scanning ${dir}`);

  // Archive original images at this level when directory didn't exist
  if (!levelExists && initImages.length) {
    await Promise.all(
      initImages.map((file) => copyFile(file, path.join(levelDir, path.basename(file))))
    );
  }

  while (true) {
    const images = await listImages(dir);
    if (images.length === 0) {
      console.log(`${indent}✅  Nothing to do in ${dir}`);
      break;
    }

    console.log(`${indent}📊  ${images.length} unclassified image(s) found`);

    // Step 1 – select ≤10
    const batch = pickRandom(images, 10);
    console.log(`${indent}🔍  Selected ${batch.length} image(s)`);

    // Step 2 – ask ChatGPT
    console.log(`${indent}⏳  Sending batch to ChatGPT…`);
    let reply = await chatCompletion({
      prompt,
      images: batch,
      model,
      curators,
    });
    console.log(`${indent}🤖  ChatGPT reply:\n${reply}`);

    let parsed;
    try {
      // Step 3 – parse decisions. Missing field_notes keys break the two-pass workflow.
      parsed = parseReply(reply, batch, {
        expectFieldNotesDiff: fieldNotes,
      });
    } catch (err) {
      if (/field_notes_/.test(err.message)) {
        // Retry once asking the model to regenerate with all keys.
        const retryPrompt = `${prompt}\nPrevious response omitted required keys—regenerate.`;
        reply = await chatCompletion({
          prompt: retryPrompt,
          images: batch,
          model,
          curators,
        });
        console.log(`${indent}🤖  Retry reply:\n${reply}`);
        parsed = parseReply(reply, batch, {
          expectFieldNotesDiff: fieldNotes,
        });
      } else {
        throw err;
      }
    }
    const { keep, aside, notes, minutes, fieldNotesDiff, fieldNotesMd } = parsed;
    if (minutes.length) {
      const minutesFile = path.join(dir, `minutes-${Date.now()}.txt`);
      await writeFile(minutesFile, minutes.join('\n'), 'utf8');
      console.log(`${indent}📝  Saved meeting minutes to ${minutesFile}`);
    }

    if (notesWriter && (fieldNotesDiff || fieldNotesMd)) {
      try {
        if (fieldNotesMd) {
          await notesWriter.writeFull(fieldNotesMd);
        } else {
          const existing = (await notesWriter.read()) || '';
          const secondPrompt = await renderTemplate(
            new URL('../prompts/field_notes_second_pass.hbs', import.meta.url).pathname,
            { prompt: basePrompt, existing, diff: fieldNotesDiff }
          );

          // Snapshot second-pass prompt for reproducibility
          await mkdir(levelDir, { recursive: true });
          await writeFile(path.join(levelDir, '.prompt.second.txt'), secondPrompt, 'utf8');

          if (showPrompt) {
            if (showPrompt === 'hash') {
              console.log(`${indent}📑  Second-pass hash ${sha256(secondPrompt)}`);
            } else if (showPrompt === 'preview') {
              const lines2 = secondPrompt.split('\n');
              const prev = lines2.slice(0, 100).join('\n');
              const trunc = lines2.length > 100 ? '\n...<truncated>...' : '';
              console.log(`${indent}📑  Second-pass preview:\n${prev}${trunc}`);
              console.log(`${indent}📑  Second-pass hash ${sha256(secondPrompt)}`);
            } else {
              console.log(`${indent}📑  Second-pass prompt:\n${secondPrompt}`);
            }
          }
          const second = await chatCompletion({
            prompt: secondPrompt,
            images: batch,
            model,
            curators,
          });
          const parsed = parseReply(second, batch, {
            expectFieldNotesMd: true,
          });
          if (parsed.fieldNotesMd) {
            await notesWriter.writeFull(parsed.fieldNotesMd);
          } else {
            console.warn(`${indent}No field_notes_md returned; diff ignored`);
          }
        }
      } catch (err) {
        console.warn(`${indent}Failed to update field notes: ${err.message}`);
      }
    }

    // Step 4 – move files
    const keepDir = path.join(dir, "_keep");
    const asideDir = path.join(dir, "_aside");
    await Promise.all([
      moveFiles(keep, keepDir, notes),
      moveFiles(aside, asideDir, notes),
    ]);

    console.log(
      `📂  Moved: ${keep.length} keep → ${keepDir}, ${aside.length} aside → ${asideDir}`
    );
  }

  // Step 5 – recurse into keepDir if both keep and aside exist
  if (recurse) {
    const keepDir = path.join(dir, "_keep");
    const asideDir = path.join(dir, "_aside");
    let keepExists = false;
    try {
      keepExists = (await stat(keepDir)).isDirectory();
    } catch {
      // ignore
    }

    if (keepExists) {
      await triageDirectory({
        dir: keepDir,
        promptPath,
        model,
        recurse,
        curators,
        contextPath,
        fieldNotes,
        depth: depth + 1,
      });
    } else {
      let keepCount = 0;
      let asideCount = 0;
      try {
        keepCount = (await listImages(keepDir)).length;
      } catch {
        // ignore
      }
      try {
        asideCount = (await listImages(asideDir)).length;
      } catch {
        // ignore
      }

      if (keepCount || asideCount) {
        const status = keepCount ? "kept" : "set aside";
        console.log(`${indent}🎯  All images ${status} at this level; stopping recursion.`);
      }
    }
  }
}
```


---

## 3. Basic System Report

Below is a snapshot of the system’s OS, architecture, date, uptime, disk usage, and memory usage.

### OS & Architecture
```
Darwin MacBook-Pro-2.local 24.4.0 Darwin Kernel Version 24.4.0: Fri Apr 11 18:33:39 PDT 2025; root:xnu-11417.101.15~117/RELEASE_ARM64_T6020 x86_64
```

### Detailed OS Version
```
ProductName:		macOS
ProductVersion:		15.4.1
BuildVersion:		24E263
```

### Current Date & Uptime
```
Sat Jul  5 01:14:50 EDT 2025
 1:14  up  7:45, 23 users, load averages: 4.52 6.43 6.71
```

### Disk Usage
```
Filesystem        Size    Used   Avail Capacity iused ifree %iused  Mounted on
/dev/disk3s1s1   7.3Ti    14Gi   284Gi     5%    425k  3.0G    0%   /
devfs            217Ki   217Ki     0Bi   100%     750     0  100%   /dev
/dev/disk3s6     7.3Ti    24Ki   284Gi     1%       0  3.0G    0%   /System/Volumes/VM
/dev/disk3s2     7.3Ti    13Gi   284Gi     5%    1.8k  3.0G    0%   /System/Volumes/Preboot
/dev/disk3s4     7.3Ti   685Mi   284Gi     1%     328  3.0G    0%   /System/Volumes/Update
/dev/disk1s2     500Mi   6.0Mi   481Mi     2%       1  4.9M    0%   /System/Volumes/xarts
/dev/disk1s1     500Mi   5.4Mi   481Mi     2%      36  4.9M    0%   /System/Volumes/iSCPreboot
/dev/disk1s3     500Mi   2.6Mi   481Mi     1%      65  4.9M    0%   /System/Volumes/Hardware
/dev/disk3s5     7.3Ti   7.0Ti   284Gi    97%    6.4M  3.0G    0%   /System/Volumes/Data
map auto_home      0Bi     0Bi     0Bi   100%       0     0     -   /System/Volumes/Data/home
/dev/disk3s1     7.3Ti    14Gi   284Gi     5%    426k  3.0G    0%   /System/Volumes/Update/mnt1
/dev/disk4s1      30Gi    13Gi    17Gi    44%       0     0     -   /Volumes/DR-70D
```

### Memory Usage
```
(Command 'free' not found on this system.)
```

### Installed RAM
```
Memory:

      Memory: 96 GB
      Type: LPDDR5
      Manufacturer: Hynix

```

## 4. Additional ML-Focused System Details

### GPU and CUDA Information
```
(nvidia-smi not found or no NVIDIA GPU installed.)

[system_profiler SPDisplaysDataType]
Graphics/Displays:

    Apple M2 Max:

      Chipset Model: Apple M2 Max
      Type: GPU
      Bus: Built-In
      Total Number of Cores: 38
      Vendor: Apple (0x106b)
      Metal Support: Metal 3
      Displays:
        Color LCD:
          Display Type: Built-in Liquid Retina XDR Display
          Resolution: 3024 x 1964 Retina
          Main Display: Yes
          Mirror: Off
          Online: Yes
          Automatically Adjust Brightness: Yes
          Connection Type: Internal
        S22C650:
          Resolution: 1920 x 1080 (1080p FHD - Full High Definition)
          UI Looks like: 1920 x 1080 @ 60.00Hz
          Mirror: Off
          Online: Yes

```

### System Compiler & Libraries
```
[gcc --version]
Apple clang version 17.0.0 (clang-1700.0.13.5)
Target: x86_64-apple-darwin24.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

[g++ --version]
Apple clang version 17.0.0 (clang-1700.0.13.5)
Target: x86_64-apple-darwin24.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

[clang --version]
Apple clang version 17.0.0 (clang-1700.0.13.5)
Target: x86_64-apple-darwin24.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

[pkg-config --modversion opencv]
Package opencv was not found in the pkg-config search path.
Perhaps you should add the directory containing `opencv.pc'
to the PKG_CONFIG_PATH environment variable
Package 'opencv' not found
OpenCV not found via pkg-config.
```

### Docker / Container Info
```
[docker --version]
Docker version 28.0.4, build b8034c0

[docker images]
Cannot connect to the Docker daemon at unix:///Users/jburkart/.docker/run/docker.sock. Is the docker daemon running?

[docker ps -a]
Cannot connect to the Docker daemon at unix:///Users/jburkart/.docker/run/docker.sock. Is the docker daemon running?
```

### Relevant Environment Variables
```
CONDA_EXE=/opt/anaconda3/bin/conda
CONDA_PREFIX=/opt/anaconda3
CONDA_PROMPT_MODIFIER=(base) 
CONDA_SHLVL=1
CONDA_PYTHON_EXE=/opt/anaconda3/bin/python
CONDA_DEFAULT_ENV=base
PATH=/opt/anaconda3/bin:/opt/homebrew/bin:/Users/jburkart/mamba/condabin:/Users/jburkart/.nvm/versions/node/v20.19.2/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/Applications/iTerm.app/Contents/Resources/utilities
```

### Ollama Models
```
NAME                  ID              SIZE      MODIFIED     
r1-1776:70b           140ea940f21d    42 GB     3 months ago    
deepseek-r1:14b       ea35dfe18182    9.0 GB    5 months ago    
deepseek-r1:8b        28f8fd6cdc67    4.9 GB    5 months ago    
deepseek-r1:7b        0a8c26691023    4.7 GB    5 months ago    
deepseek-r1:1.5b      a42b25d8c10a    1.1 GB    5 months ago    
deepseek-r1:70b       0c1615a8ca32    42 GB     5 months ago    
deepseek-r1:32b       38056bbcbb2d    19 GB     5 months ago    
deepseek-r1:latest    0a8c26691023    4.7 GB    5 months ago    
```

Overview generation complete. The file 'project-overview.txt' has been created.
